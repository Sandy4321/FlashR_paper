\section{Experimental evaluation}
We evaluate the performance of FlashMatrix with statistics and machine learning
applications both in memory and on SSDs. We compare their performance with
the implementations in Spark MLlib \cite{mllib}, a highly-optimized parallel
machine learning library, and the C and FORTRAN implementations in the R framework.
We further illustrate the effectiveness of the optimizations deployed in
FlashMatrix when running both in memory and on SSDs.

We conduct experiments on a non-uniform memory architecture machine with
four Intel Xeon E7-4860 processors, clocked at 2.6 GHz, and 1TB memory of
DDR3-1600. Each processor has 12 cores. The machine has three LSI SAS 9300-8e
host bus adapters (HBA) connected to a SuperMicro storage chassis, in which
24 OCZ Intrepid 3000 SSDs are installed. The 24 SSDs together are capable of
delivering 12 GB/s for read and 10 GB/s for write at maximum. The machine runs
Linux kernel v3.13.0. By default, we use 48 threads for both in-memory and
out-of-core execution of FlashMatrix.

\subsection{Statistics and Machine learning applications} \label{sec:apps}
We implement multiple important applications in the field of statistics and
machine learning. These applications have various computation complexity but
have the same I/O complexity (Table \ref{tbl:algs}). We implement these
applications completely with the R interface of FlashMatrix and
rely on FlashMatrix to perform computation in parallel and out of core.
\begin{itemize}
	\item Multivariate statistical summary: this computes column-wise minimum,
		maximum, mean, L1 norm, L2 norm, the number of non-zero values and
		variance on a data matrix.
	\item Correlation: this computes pair-wise Pearson's correlation \cite{cor}
		among multiple series of data and is commonly used in statistics.
	\item Singular value decomposition (SVD) factorizes a matrix into
		three matrices: $U$, $\Sigma$ and $V$ such that $A=U \Sigma V^T$, where
		$U$ and $V$ are orthonormal matrices and $\Sigma$ is a diagonal
		matrix with non-negative diagonals in descending order. SVD is commonly
		used for dimension reduction. In the experiments, we compute 10 singular
		values.
	\item K-means \cite{kmeans} is an iterative algorithm of partitioning a set
		of data points into $k$ clusters so that each cluster has minimal mean
		of distances between the data points in the cluster and the cluster
		center. K-means
		is one of the most popular clustering algorithms and is identified as
		one of the top 10 data mining algorithms \cite{top10}. In the experiments,
		we run k-means to split a dataset into 10 clusters by default.
	\item Gaussian Mixture Model (GMM) \cite{gmm} is another iterative clustering
		algorithm that assumes data points are sampled from a mixture of
		Gaussian distributions and use expectation maximization (EM) \cite{gmm}
		algorithm to fit the model. This algorithm is also identified as one
		of the top 10 data mining algorithms \cite{top10}. In the experiments,
		we run GMM to split a dataset into 10 clusters by default.
\end{itemize}

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Application & Computation & I/O \\
\hline
Summary & $O(n \times p)$ & $O(n \times p)$ \\
\hline
Correlation & $O(n \times p^2)$ & $O(n \times p)$ \\
\hline
SVD & $O(n \times p^2)$ & $O(n \times p)$ \\
\hline
K-means (1 iteration) & $O(n \times p \times k)$ & $O(n \times p)$ \\
\hline
GMM (1 iteration) & $O(n \times p^2 \times k)$ & $O(n \times p + n \times k)$ \\
\hline
\end{tabular}
\normalsize
\end{center}
\caption{The computation and I/O complexity of the algorithms for the five
	applications. $n$ is the number of data points in the dataset, $p$ is
	the number of the attributes and $k$ is the number of clusters k-means and
GMM partition the dataset.}
\label{tbl:algs}
\end{table}

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Data Matrix & n & p & size \\
\hline
Friendster-32 \cite{friendster} & 65M & 32 & 16GB \\
\hline
MixGaussian-1B & 1B & 32 & 251GB \\
\hline
Random-65M & 65M & 8-512 & 4-248GB \\
\hline
\end{tabular}
\normalsize
\end{center}
\caption{Datasets for performance evaluation.}
\label{tbl:data}
\end{table}

K-means and GMM typically run on a dataset with a small number of attributes
due to curse of dimensionality \cite{Jain00} while the other applications may
be applied to datasets with various numbers of attributes. We use the datasets
in Table \ref{tbl:data} for performance evaluation. We run k-means and GMM on
the Friendster-32 matrix, constructed from 32 eigenvectors of the Friendster
graph \cite{friendster}, as well as the MixGaussian-1B matrix with one billion data
points and 32 attributes, sampled from 10 mixtures of Gaussian distributions
with the identity covariance matrix and different means. We measure the performance
of the other three applications on all of the matrices in Table \ref{tbl:data},
including the random matrices with 65 million rows and the number of columns
varying from 8 to 512.

\subsection{Comparative performance of FlashMatrix}

We compare the performance of the FlashMatrix implementations with the ones in
Spark MLlib \cite{mllib} and the R framework. We run the MLlib implementations
with their native Scala interface and use a very large heap size to ensure that
all input data is cached in memory. We use 48 threads for both FlashMatrix and
MLlib to run on the MixGaussian-1B matrix.
The R framework provides C implementations for correlation, SVD and k-means.
The R package mclust \cite{mclust} provides a FORTRAN implementation of GMM.
These implementations run in a single thread. We run the FlashMatrix
implementations in a single thread and compare their performance with the C and
FORTRAN implementations on the Friendster-32 matrix.

\begin{figure}
	\centering
	\footnotesize
	\vspace{-15pt}
	\begin{subfigure}{.5\textwidth}
		\include{FM.vs.spark}
		\label{perf:rt}
		\vspace{-15pt}
		\caption{Runtime}
	\end{subfigure}

	%\vspace{-5pt}
	\begin{subfigure}{.5\textwidth}
		\include{FM.vs.spark.mem}
		\label{perf:mem}
		\vspace{-15pt}
		\caption{Memory consumption}
	\end{subfigure}
	\caption{The performance and memory consumption of FlashMatrix both
		in memory (FM-IM) and on SSDs (FM-EM) compared with Spark MLlib
		on the MixGaussian-1B matrix.}
	\label{perf:fm}
\end{figure}

%\begin{figure}
%	\begin{center}
%		\footnotesize
%		\vspace{-15pt}
%		\include{FM.vs.spark}
%		\vspace{-15pt}
%		\caption{The performance of FlashMatrix both in memory and on SSDs
%			compared with Spark MLlib on a matrix with one billion rows and
%		32 columns.}
%		\label{perf:fm}
%	\end{center}
%\end{figure}

FlashMatrix both in memory and on SSDs outperforms Spark MLlib significantly
in all applications (Figure \ref{perf:fm} (a)). For some applications
such as correlation, SVD and GMM, even though both FlashMatrix and MLlib
implementations heavily rely on BLAS for matrix multiplication, FlashMatrix
outperforms MLlib significantly owing to our heavy optimizations on GenOps
such as aggressive matrix operation fusion and VUDFs. In contrast, MLlib
materializes operations such as aggregation separately and implements
non-BLAS operations with Scala.
%SVD is the only application whose FlashMatrix
%implementation does not outperform the one in MLlib when being executed out of
%core because this application requires to read the entire matrix multiple times
%and output a large matrix.

%\begin{figure}
%	\begin{center}
%		\footnotesize
%		\vspace{-15pt}
%		\include{FM.vs.spark.mem}
%		\vspace{-15pt}
%		\caption{The memory consumption of FlashMatrix in memory and on SSDs
%		as well as Spark MLlib on a matrix with one billion rows and 32 columns.}
%		\label{perf:mem}
%	\end{center}
%\end{figure}

Even though FlashMatrix provides a matrix-oriented functional programming
interface, it easily scales to datasets with billions of data points and its
scalability is bound by the capacity of SSDs (Figure \ref{perf:fm} (b)).
For out-of-core execution, FlashMatrix keeps large matrices on
SSDs and has a very small memory footprint. The functional programming
interface generates a new matrix in each matrix operation, which potentially
leads to high memory consumption. Owing to lazy evaluation,
FlashMatrix does not store majority of matrices in the computation physically.
As such, its in-memory execution barely increases memory consumption from
the minimum memory requirement of the applications. This indicates that
the out-of-core execution consumes small space on SSDs, which leads to
very high scalability.

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{FM.vs.R}
		\vspace{-10pt}
		\caption{The performance of FlashMatrix in a single thread both in
			memory (FM-IM) and on SSDs (FM-EM) compared with the C and FORTRAN
		implementations in the R framework on the Friendster-32 matrix.}
		\label{fig:fmR}
	\end{center}
\end{figure}

FlashMatrix running both in memory and on SSDs significantly outperforms R
even with a single thread in all of these applications (Figure \ref{fig:fmR}).
We exclude statistic summary in the experiment because R does not provide
a C or FORTRAN implementation of computing the same statistics. The performance
results indicate that FlashMatrix executes R code efficiently to even outperform
some optimized C and FORTRAN implementations when processing large datasets.

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{speedup}
		\vspace{-10pt}
		\caption{The speedup of FlashMatrix with multithreading both in memory (IM)
		and on SSDs (EM).}
		\label{fig:speedup}
	\end{center}
\end{figure}

The in-memory execution of FlashMatrix achieves almost linear speedup in all
applications while the out-of-core execution only
starts to flatten out after 32 threads (Figure \ref{fig:speedup}). Owing to
operation fusion in CPU cache, FlashMatrix significantly reduces data movement
between CPU and main memory. As such, memory bandwidth is no longer
the bottleneck and the computation speeds up linearly with more CPU cores
when the applications run in memory. The SSDs deliver about 10GB/s I/O throughput
and are the bottleneck for the applications with lower computation complexity.
GMM still speeds up almost linearly even when running on SSDs, due to its high
computation complexity. The performance results in Figure \ref{fig:fmR} and Figure
\ref{fig:speedup} indicate that FlashMatrix can potentially execute R code with
performance comparable to parallel C or FORTRAN implementations.

%\begin{figure}
%	\begin{center}
%		\footnotesize
%		\vspace{-15pt}
%		\include{scale}
%		\vspace{-10pt}
%		\caption{The scalability and performance of k-means in different
%		frameworks.}
%		\label{fig:scale}
%	\end{center}
%\end{figure}

\subsection{Performance of FlashMatrix in memory and on SSDs}

We further measure the in-memory and external-memory performance of FlashMatrix
thoroughly with different datasets and
different parameters. We run the first three applications on random matrices
with the number of columns varying from 8 to 512. We run k-means
and GMM on the Friendster-32 matrix and vary the number of clusters from 2 to 64.

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{IM.vs.EM.stat}
		\vspace{-10pt}
		\caption{The relative performance of FlashMatrix on SSDs for
			statistics computation on random-65M matrices with the number of
			columns varying from 8 to 512, normalized by its performance
		in memory.}
		\label{perf:stat}
	\end{center}
\end{figure}

As the number of attributes in the datasets or the number of clusters increases,
the performance gap between in-memory and external-memory execution
narrows and eventually the external-memory performance gets almost 100\%
of in-memory performance (Figure \ref{perf:stat} and \ref{perf:clust}).
This observation conforms with the computation and I/O complexity of
the applications in Table \ref{tbl:algs}. When the number of attributes
in the dataset gets larger, the computation of matrix multiplication in
correlation and SVD grows more rapidly than I/O and eventually CPU becomes
the bottleneck. The current implementation of correlation requires an additional
pass on the input matrix to compute column-wise mean values, which results in
lower external-memory performance. Similarly,
the computation of k-means and GMM increases more rapidly than I/O and
these applications are dominated by their CPU computation as the number
of clusters gets larger. Given the I/O throughput of 10 GB/s, the applications
do not require many attributes or clusters to have their external-memory
performance close to their in-memory performance.

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{IM.vs.EM.clust}
		\vspace{-10pt}
		\caption{The relative performance of FlashMatrix on SSDs for
			clustering algorithms with different numbers of clusters, normalized
		by its performance in memory.}
		\label{perf:clust}
	\end{center}
\end{figure}

\subsection{Effectiveness of optimizations}

In this section, we illustrate the effectiveness of our memory and CPU
optimizations in FlashMatrix. To reduce memory overhead, we focus on three
main optimizations: \textit{(i)} reusing memory chunks for new in-memory
matrices and I/O access to reduce large
memory allocation, \textit{(ii)} matrix operation fusion in main
memory to reduce data movement between SSDs and main memory, \textit{(iii)}
matrix operation fusion in CPU cache to reduce data movement between main
memory and CPU cache. To reduce computation overhead, we illustrate
the effectiveness of using VUDFs.

Each memory optimization has significant performance improvement on most of
the applications when they are executed on SSDs (Figure \ref{perf:opts}).
Due to the space limit, we only illustrate the effectiveness of the memory
optimizations in out-of-core execution. Operation fusion in main memory achieves
the highest performance improvement in almost all applications, even in GMM,
which has the highest asymptotic computation complexity. Even though the SSDs
deliver 10GB/s I/O throughput, materializing every matrix operation separately
causes SSDs to be the main bottleneck in the system.
Fusing matrix operations in memory significantly reduces the burden on SSDs and
improves performance by a large factor. Operation fusion in the CPU cache also
has very positive performance impact on some applications even when
the applications run on SSDs. This suggests that with sufficient I/O optimizations,
many machine learning applications that run on fast SSDs can be bottlenecked by
the bandwidth of main memory, instead of I/O. Even though it is less noticeable,
reducing large memory allocation improves I/O performance and almost doubles
the overall performance of all applications.

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{opts.EM}
		\vspace{-10pt}
		\caption{The effectiveness of memory optimizations on different
			applications when they are executed on SSDs. The three memory
		optimizations are applied to FlashMatrix incrementally.}
		\label{perf:opts}
	\end{center}
\end{figure}

Using VUDFs improves the performance of the applications that rely on GenOps
for computation (Figure \ref{perf:opts_CPU}). In this experiment, the base
implementations deploy all memory optimizations to avoid memory from being
the bottleneck of the system,
and invoke functions on individual elements in
matrices. The FlashMatrix implementations of computing statistical summary
and k-means solely rely on GenOps. Therefore, their performance is almost doubled
when using VUDFs. The main computation in correlation and GMM is matrix
multiplication, but they still rely on GenOps for the remaining computation.
As such, using VUDFs help their performance. SVD solely uses matrix
multiplication, so switching VUDFs has no performance impact.

\begin{figure}
	\begin{center}
		\footnotesize
		\vspace{-15pt}
		\include{opts.CPU}
		\vspace{-10pt}
		\caption{The effectiveness of using VUDFs on different applications
		when they are executed in memory.}
		\label{perf:opts_CPU}
	\end{center}
\end{figure}
