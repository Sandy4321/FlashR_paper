\section{Experimental evaluation}
We evaluate the performance of FlashMatrix with statistics and machine learning
applications. We implement these applications with the R interface of FlashMatrix
and measure their performance both in memory and on SSDs. We compare their
performance with the implementations in Spark MLlib \cite{mllib}, a high-
optimized parallel machine learning library. We further
illustrate the effectiveness of the optimizations deployed in FlashMatrix
when running both in memory and on SSDs.

We conduct experiments on a non-uniform memory architecture machine with
four Intel Xeon E7-4860 processors, clocked at 2.6 GHz, and 1TB memory of
DDR3-1600. Each processor has 12 cores. The machine has three LSI SAS 9300-8e
host bus adapters (HBA) connected to a SuperMicro storage chassis, in which
24 OCZ Intrepid 3000 SSDs are installed. The 24 SSDs together are capable of
delivering 12 GB/s for read and 10 GB/s for write at maximum. The machine runs
Linux kernel v3.13.0. We use 48 threads for both in-memory and out-of-core
execution of FlashMatrix as well as Spark.

\subsection{Statistics and Machine learning applications} \label{sec:apps}
We implement multiple important applications in the field of statistics and
machine learning. These applications are implemented completely with R and
rely on FlashMatrix to perform computation in parallel and out of core.
\begin{itemize}
	\item Multivariant statistical summary: this computes minimum, maximum,
		mean, L1 norm, L2 norm, the number of non-zero values
		and variance of each random variable for a given set of observations.
	\item Correlation: this computes Pearson's correlation \cite{} between
		two series of observations and is commonly used in statistics.
	\item Singular value decomposition (SVD) \cite{} factorizes a matrix into
		three matrices: $U$, $\Sigma$ and $V$ such that $A=U \Sigma V^T$, where
		both $U$ and $V$ are orthonormal matrices and $\Sigma$ is a diagonal
		matrix with non-negative diagonals in descending order. SVD is commonly
		used for dimension reduction.
	\item KMeans \cite{kmeans} is an iterative algorithm of partitioning a set
		of observations into $k$ clusters
		so that each data point belongs to the cluster with minimal mean. KMeans
		is one of the most popular clustering algorithms and is identified as
		one of the top 10 data mining algorithms \cite{top10}.
	\item Gaussian Mixture Model (GMM) \cite{gmm} is also an iterative
		clustering algorithm that assumes observations are sampled under
		a mixture of Gaussian distributions. We implement the expectation
		maximization (EM) \cite{em} algorithm to fit
		the mixture of Gaussian distributions. This algorithm is also identified
		as one of the top 10 data mining algorithms \cite{top10}.
\end{itemize}

The algorithms have various computation complexity but have the same I/O
complexity (Table \ref{tbl:algs}). Computing statistical summary
has the lowest computation complexity, while GMM has the highest asymptotic
computation complexity. The first three applications require to read the entire
dataset once to perform computation and each iteration of KMeans requires to
read the entire dataset once. Despite lazy evalution, the FlashMatrix
implementation of GMM requires to read the dataset multiple times in each
iteration.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Application & Computation & I/O \\
\hline
Summary & $O(n \times p)$ & $O(n \times p)$ \\
\hline
Correlation & $O(n \times p^2)$ & $O(n \times p)$ \\
\hline
SVD & $O(n \times p^2)$ & $O(n \times p)$ \\
\hline
KMeans (1 iteration) & $O(n \times p \times k)$ & $O(n \times p)$ \\
\hline
GMM (1 iteration) & $O(n \times p^2 \times k)$ & $O(n \times p + n \times k)$ \\
\hline
\end{tabular}
\normalsize
\end{center}
\caption{The computation and I/O complexity of the algorithms for the five
	applications. $n$ is the number of data points in the dataset, $p$ is
	the number of the features and $k$ is the number of clusters KMeans and
GMM partition the dataset to.}
\label{tbl:algs}
\end{table}

Typically, KMeans and GMM run on a dataset with a small number of features
while the other applications may be applied to datasets with various numbers
of features. For performance evaluation, we run KMeans and GMM on a matrix
with 65 million rows and 32 columns, constructed from 32 eigenvectors from
the Friendster graph \cite{friendster}. Running KMeans on eigenvectors is
an important step of spectral clustering \cite{} to partition vertices to
clusters. We measure the performance of the other three applications on
random matrices with 64 million rows and vary the number of columns from 8
to 512.

\subsection{Performance of FlashMatrix in memory and on SSDs}

We compare the in-memory and external-memory performance of the FlashMatrix
implementations of the applications thoroughly with different datasets and
different settings. For the first three applications, we vary the number of
features from 8 to 512. For KMeans and GMM, we vary the number of clusters
from 2 to 64.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{IM.vs.EM.stat}
		\caption{The relative external-memory performance of FlashMatrix for
			statistics computation on a dataset with the number of features
		varying from 8 to 512, normalized by its in-memory performance.}
		\label{perf:stat}
	\end{center}
\end{figure}

As the number of features in the datasets or the number of clusters increases,
the performance gap between in-memory and external-memory execution
narrows and eventually the external-memory performance gets almost 100\%
of in-memory performance (Figure \ref{perf:stat} and \ref{perf:clust}).
This observation conforms with the computation and I/O complexity of
the applications in Table \ref{tbl:algs}. When the number of features
in the dataset gets larger, computation in correlation and SVD grows more
rapidly than I/O and eventually CPU becomes
the bottleneck. Similarly, the computation of KMeans and GMM increases
more rapidly than I/O and get dominated by their CPU computation as the number
of clusters gets larger. Given the I/O throughput of 10 GB/s, it does not
require many features or clusters to have the applications bottlenecked by
CPU.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{IM.vs.EM.clust}
		\caption{The relative external-memory performance of FlashMatrix for
			clustering algorithms with different numbers of clusters, normalized
		by its in-memory performance.}
		\label{perf:clust}
	\end{center}
\end{figure}

\subsection{Performance of FlashMatrix vs. Spark MLlib}

We further compare the performance of FlashMatrix implementations with the ones
in Spark MLlib \cite{mllib}. We implement the applications with
the same algorithms used by Spark MLlib. For fair comparison, we runs the Spark
implementations with their native Scala interface and use a very large heap size
to ensure that all input data is cached in memory.

The applications in FlashMatrix significantly outperform Spark MLlib (Figure
\ref{perf:fm}). When the applications require more computation, the out-of-core
execution of FlashMatrix significantly outperforms Spark MLlib. Multivariant
statistical summary is the only application that does not outperform Spark
when executed out of core because its out-of-core execution is bottlenecked
by I/O. For applications such as correlation and GMM, even though both FlashMatrix
and Spark implementations heavily rely on BLAS for matrix multiplication,
FlashMatrix can still significantly outperform Spark thanks to matrix operation
fusion and two-level partitioning to reduce data movement between memory and CPU.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{FM.vs.spark}
		\caption{The performance of FlashMatrix both in memory and on SSDs
		compared with Spark.}
		\label{perf:fm}
	\end{center}
\end{figure}

\dz{TODO}
Array-oriented functional languages can consume a lot of memory. Here we show
that FlashMatrix actually doesn't consume much memory and the out-of-core
execution further reduce memory consumption.

\subsection{Performance of FlashMatrix vs. R}
In this section, we compare the performance of FlashMatrix with R on the matrix
with 65 million rows and 32 columns. To have
a fair comparison, we run FlashMatrix in a single thread. In addition, we
measure the speedup of FlashMatrix with multithreading. The R framework
provides C implementations for correlation, SVD and KMeans. We use R functions
to compute statistic summary of each attribute except minimum and maximum
because R does not provide an efficient way of computing minimum and maximum
of columns of a matrix.

\dz{I need to GMM in R.}

\begin{figure}
	\begin{center}
		\footnotesize
		\include{FM.vs.R}
		\caption{The performance of in-memory FlashMatrix in a single thread
		compared with R.}
		\label{fig:fmR}
	\end{center}
\end{figure}

FlashMatrix significantly outperforms R even with a single thread in these
applications (Figure \ref{fig:fmR}). Even though the FlashMatrix implementation
of statistic summary computes more statistical values, it still outperforms
the R implementation. The factor that the FlashMatrix
implementations of correlation, SVD and KMeans outperform the C implementations
in R indicates that FlashMatrix is able to run R code very efficiently
and has performance comparable to or even outperforms some C implementations.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{speedup}
		\caption{The speedup of in-memory FlashMatrix with multithreading.}
		\label{fig:speedup}
	\end{center}
\end{figure}

When running in memory, all FlashMatrix implementations are able to speed up
almost linearly with more CPU cores owing to reduced data movement between
CPU and main memory (Figure \ref{fig:speedup}).

\subsection{Effectiveness of optimizations}

In this section, we measure the effectiveness of optimizations in FlashMatrix,
i.e., matrix operation fusion to reduce data movement between CPU and SSDs and
vectorized user-defined functions (VUDF) to reduce the overhead of function
calls. We illustrate the effectiveness of operation fusion in both main memory
and CPU cache. We use GMM to illustrate the effectiveness of matrix
operation fusion because this application allows aggressive operation fusion.

Matrix operation fusion has very positive impact on both in-memory and
external-memory performance of GMM (Figure \ref{perf:mem_move}). Even though
GMM has the highest computation overhead among all applications in Section
\ref{sec:apps}, materializing every matrix operation on SSDs causes significant
performance degradation and SSDs are the main bottleneck. Fusing matrix
operations in memory significantly reduces the burden on SSDs and improves
performance by a factor of 5. Operation fusion in main memory also has noticeable
performance improvement when GMM runs in memory, although the impact is much
lower, owing to reduced memory allocation overhead. Operation fusion in
the CPU cache also has very positive performance improvement on in-memory
and external-memory execution of GMM by 30\% and 28\%, respectively, on top of
the improvement from fusion in main memory.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{opts_mem_move}
		\caption{The effectiveness of matrix operation fusion in main memory
			(mem-fuse) and in CPU cache (cache-fuse). The operation fusion in
		CPU cache is applied on top of the fusion in main memory.}
		\label{perf:mem_move}
	\end{center}
\end{figure}

Overhead of function calls.
