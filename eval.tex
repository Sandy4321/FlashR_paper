\section{Experimental evaluation}
We evaluate the efficiency of FlashR on statistics and machine learning
algorithms both in memory and on SSDs. We compare the R implementations of
these algorithms with the ones in two highly-optimized parallel machine learning
libraries H2O \cite{h2o} and Spark MLlib \cite{mllib}. We further use FlashR
to accelerate existing R functions in the MASS package and compare with
Revolution R Open \cite{rro}.

We conduct experiments on a NUMA machine with
four Intel Xeon E7-4860 2.6 GHz processors and 1TB of 
DDR3-1600 memory. Each processor has 12 cores. The machine has three LSI SAS 9300-8e
host bus adapters connected to
24 OCZ Intrepid 3000 SSDs. The SSDs are capable of
12 GB/s for read and 10 GB/s for write. The machine runs
Linux kernel v3.13.0. By default, we use 48 threads. 
We use Spark v2.0.1 and R v3.3.2. We use ATLAS 3.10.1 as the default BLAS library.

\subsection{Benchmark algorithms}
We benchmark FlashR with some commonly used algorithms in Table
\ref{tbl:algs}. Like the algorithms shown in Section \ref{sec:apps},
we implement these algorithms completely with the R code and
rely on FlashR to execute them in parallel and out of core.

\noindent \textbf{Correlation} computes pair-wise Pearson's correlation
\cite{cor} and is commonly used in statistics.

\noindent \textbf{Principal Component Analysis (PCA)} computes uncorrelated
variables from a large dataset. PCA is commonly used for dimension reduction
in many data analysis. We compute PCA by computing eigenvalues on the Gramian
matrix $A^T A$ of the input matrix $A$.

\noindent \textbf{Naive Bayes} is a classifier that applies Bayes' theorem
with the "naive" assumption of independence between every pair of features.
Our implementation assumes data follow the normal distribution.

\noindent \textbf{Logistic regression} is a classifier that applies
the sigmoid functions on the linear model. We use the Newton-CG algorithm
to optimize logistic regression.

\noindent \textbf{K-means \cite{kmeans}} is a clustering algorithm that
partitions data points into $k$ clusters so that each cluster has minimal
mean of distances between the data points and the cluster center. 
In the experiments, we run k-means to split a dataset into 10 clusters
by default.

\noindent \textbf{Multivariate Normal Distribution (mvrnorm)} generates
samples from the specified multivariate normal distribution. We use
the implementation in the MASS package.

\noindent \textbf{Linear discriminant analysis (LDA)} is a linear classifier
that assumes the normal distribution with a different mean for each class
but sharing the same covariance matrix among classes. Our LDA is adapted from
the one in the R MASS package only with some trivial modifications.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Algorithm & Computation & I/O \\
\hline
Correlation & $O(n \times p^2)$ & $O(n \times p)$ \\
\hline
PCA & $O(n \times p^2)$ & $O(n \times p)$ \\
\hline
Naive Bayes & $O(n \times p)$ & $O(n \times p)$ \\
\hline
mvrnorm & $O(n \times p^2)$ & $O(n \times p)$ \\
\hline
LDA & $O(n \times p^2)$ & $O(n \times p)$ \\
\hline
Logistic regression  (1 iteration) & $O(n \times p^2)$ & $O(n \times p)$ \\
\hline
K-means (1 iteration) & $O(n \times p \times k)$ & $O(n \times p)$ \\
\hline
\end{tabular}
\normalsize
\end{center}
\caption{Computation and I/O complexity. $n$ is the number of data points, $p$
is the number of the features in a point, and $k$ is the number of clusters.
	\dz{I need to analyze the computation complexity more carefully.}}
\label{tbl:algs}
\end{table}

These algorithms have various ratios of computation complexity and I/O complexity
(Table \ref{tbl:algs}) to thoroughly evaluate perforamnce of FlashR on SSDs.
The first five algorithms require a constant number of passes over the input
matrix. Logistic regression and K-means run iteratively and, thus, we show
their computation and I/O complexity in a single iteration.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Data Matrix & n & p & size \\
\hline
PageGraph-32ev \cite{webgraph} & 3.5B & 32 & 800GB \\
\hline
Criteo \cite{criteo} & 4.3B & 40 & 637GB \\
\hline
PageGraph-32ev-sub \cite{webgraph} & 336M & 32 & 80GB \\
\hline
Criteo-sub \cite{criteo} & 325M & 40 & 48GB \\
\hline
\end{tabular}
\normalsize
\end{center}
\caption{Datasets ($n \times p$ matrices). PageGraph-32ev-sub and
	Criteo-sub are the subsets of PageGraph-32ev and Criteo. They
	have about one tenth of the whole datasets.}
\label{tbl:data}
\vspace{-5pt}
\end{table}

We benchmark FlashR with two real-world datasets with billions of data points
(Table \ref{tbl:data}). The Criteo dataset has over four billion data points
with binary labels (click vs. no-click), used for advertisement click
prediction \cite{criteo}. PageGraph-32ev are 32 singular vectors that we
computed on the largest connected component of the Page graph \cite{webgraph}.
To compare FlashR with other frameworks, we take part of the datasets to
create smaller datasets. PageGraph-32ev-sub is the first 336 million data points
of the PageGraph-32ev dataset. Criteo-sub contains the data points collected
on the first two days, which is about one tenth of the whole dataset.

%\vspace{-8pt}
\subsection{Comparative performance}
%\vspace{-4pt}
We evaluate FlashR against H2O \cite{h2o} and Spark MLlib \cite{mllib} as well
as Revolution R Open \cite{rro}. We compare these frameworks in a large
parallel machine with 48 CPU cores. In addition, we run FlashR in the Amazon
cloud and compare its speed in a single EC2 instance with Spark MLlib in
a cluster.

In this section, we compare the R code of the machine learning algorithms
in FlashR with the implementations in H2O and Spark MLlib.
We give H2O and MLlib a large heap size (500GB) to ensure that all data are
cached in memory. We run logistic regression in H2O and MLlib with Limited-memory
Broyden-Fletcher-Goldfarb-Shanno algorithm (LBFGS) \cite{lbfgs} to get fast
convergence. We also use FlashR to parallelize some functions in the R
MASS package and compare their speed with Revolution R. In all experiments,
we use 48 threads for all frameworks.

\begin{figure}
  \vspace{-5pt}
	\centering
	\footnotesize
	\begin{subfigure}{.5\textwidth}
		\includegraphics{FlashMatrix_figs/FlashR-vs-dist.eps}
		\label{perf:para}
		\caption{In a large parallel machine with 48 CPU cores.}
	\end{subfigure}

	\vspace{3pt}
	\begin{subfigure}{.5\textwidth}
		\includegraphics{FlashMatrix_figs/FlashR-vs-dist-EC2.eps}
		\label{perf:cloud}
		\caption{In the Amazon cloud. FlashR-IM and FlashR-EM run on one
			EC2 i2.8xlarge instance (16 CPU cores) and Spark MLlib runs
		on a cluster of four EC2 c4.8xlarge instances (72 CPU cores).}
	\end{subfigure}
	\vspace{-8pt}
	\caption{The normalized runtime of FlashR in memory (FlashR-IM) and
	on SSDs (FlashR-EM) compared with H2O and Spark MLlib. Correlation is not
	available in H2O. We run k-means on the PageGraph-32ev-sub dataset and
	all other algorithms on the Criteo-sub dataset.}
	\label{perf:rt}
  \vspace{-10pt}
\end{figure}

FlashR outperforms H2O and Spark MLlib significantly on all algorithms
(Figure \ref{perf:para}) in the large parallel machine with 48 CPU cores.
FlashR running in memory achieves 3 to 10 times performance gain when compared
with MLlib, and 2.5 to 7 times performance gain when compared with H2O.
When running on SSDs, FlashR achieves at least half the speed of running in
memory. Even though logistic regression in FlashR and the ones in H2O and MLlib
uses different algorithms for numerical optimization, the number of iterations
required by the optimization algorithms to converge to similar loss is fairly
close (10 iterations for Newton-CG and 14 iterations for LBFGS), while computation
complexity of Newton-CG is much higher than LBFGS.
Even though most of the algorithms, especially correlation and PCA,
relies on BLAS for matrix multiplication to achieve speed, H2O and MLlib
implement non-BLAS operations with Java and Scala, respectively. Furthermore,
MLlib materializes operations such as aggregation separately. In contrast,
FlashR fuses matrix operations and performs two-level partitioning to
minimize data movement in the memory hierarchy, and keeps data in the local
memory to achieve high memory bandwidth.

We further evaluate the speed of FlashR on Amazon EC2 cloud and compare it with
Spark MLlib on an EC2 cluster (Figure \ref{perf:cloud}). We run FlashR on
the largest storage-optimized
EC2 instance (i2.8xlarge), which has 16 CPU cores, 244GB RAM and 6.4TB SSD storage,
and run Spark MLlib on the largest compute-optimized EC2 instances (c4.8xlarge),
each of which has 18 CPU cores, 60GB RAM and is connected with 10Gbps network.
Spark MLlib needs at least 4 c4.8xlarge instances to process the datasets
(PageGraph-32ev-sub and Criteo-sub).
Even though Spark MLlib has 4.5 as much computation power as FlashR, FlashR
still matches the speed of Spark MLlib and even outperform it.

\begin{figure}[b]
  \vspace{-10pt}
	\begin{center}
		\footnotesize
		\includegraphics{FlashMatrix_figs/FlashR-vs-RRO.eps}
		\caption{In-memory (FlashR-IM) and out-of-fore (FlashR-EM) FlashR
		compared with Revolution R Open on a data matrix with one million rows
		and one thousand columns.}
		\label{fig:fmR}
	\end{center}
  \vspace{-15pt}
\end{figure}

FlashR running both in memory and on SSDs outperforms Revolution R Open by more
than an order of magnitude even on a small dataset ($n=1,000,000$ and $p=1000$)
(Figure \ref{fig:fmR}).
Revolution R Open uses Intel MKL to parallelize matrix multiplication. As such,
we only compare the two frameworks with computations that use matrix
multiplication heavily. Both FlashR and Revolution R Open run the mvrnorm
and LDA implementations from the MASS package. For simple matrix operations such as crossprod,
FlashR has slightly higher speed than Revolution R Open. For more complex
computation, the speed gap between FlashR and Revolution R gets larger when
computation gets more complex. Even though matrix multiplication
is the most computation-intensive operation in an algorithm, it is insufficient
to only paralleize matrix multiplication to achieve high efficiency.

\subsection{Scalability}

We show the scalability of FlashR on the billion-scale datasets in Table
\ref{tbl:data}. In this experiments, we run the iterative algorithms
(k-means and logistic regression) until they converge on the datasets.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|}
\hline
	& Runtime (s) & Memory (GB) \\
\hline
Correlation & $91.23$ & $1.5$ \\
\hline
PCA & $136.71$ & $1.5$ \\
\hline
NaiveBayes & $76.55$ & $3$ \\
\hline
Logistic regression & $4154.40$ & $26$ \\
\hline
k-means & $1110.82$ & $28$ \\
\hline
%PageRank & $xx$ & $xx$ \\
%\hline
%Spectral embedding & $xx$ & $xx$ \\
%\hline
\end{tabular}
\normalsize
\end{center}
\caption{The runtime and memory consumption of FlashR on the billion-scale
	datasets on the 48 CPU core machine. The runtime of logistic regression
	and k-means is measured when the two algorithms converge.}
\label{tbl:scale}
\end{table}

Even though we process the datasets with billions of data points in a single
machine, none of the algorithms are prohibitively expensive. Simple algorithms,
such as Naive Bayes and PCA, require one or two passes over the datasets and
take only one or two minutes to complete. Iterative
algorithms, such as logistic regression and k-means, take about $10-20$
iterations to converge and the total computation time is about one hour or
less.

Despite the matrix-oriented functional programming interface of FlashR,
it scales to datasets with billions of data points easily with negligible
amounts of memory when running out of core.
Its scalability is bound by the capacity of SSDs. The functional programming
interface generates a new matrix in each matrix operation, which potentially
leads to high memory consumption. Thanks to lazy evaluation and virtual matrices,
FlashR only needs to materialize the small matrices to effectively reduce
memory consumption and streams data from SSDs for computation.
%Owing to lazy evaluation, FlashR does not store majority of matrices in
% the computation physically. As such, its in-memory 
%out-of-core execution barely increases memory consumption from
%the minimum memory requirement of the algorithms. 
%This indicates that the out-of-core execution consumes small space on SSDs, which leads to
%very high scalability.

%\vspace{-8pt}
\subsection{Computation complexity versus I/O complexity}
%\vspace{-4pt}
We further compare the speed of FlashR in memory and in external memory
for algorithms with different computation and I/O complexities.
We pick three algorithms from Table \ref{tbl:algs}: \textit{(i)} Naive Bayes,
whose has the same computation complexity as its I/O complexity, \textit{(ii)}
correlation, whose computation complexity grows quadratically on $p$ while
its I/O complexity grows linear on $p$, \textit{(iii)} k-means, whose computation
complexity grows linearly with $k$ while its I/O complexity does not have
the factor. We run the first two algorithms on datasets with $n=100M$ and $p$
varying from 8 to 512. We run k-means on a dataset with $p=100M$ and $p=32$
and vary the number of clusters from 2 to 64.

\begin{figure}[t]
	\begin{center}
		\footnotesize
		\includegraphics{FlashMatrix_figs/IM-vs-EM-stat.eps}
		\includegraphics{FlashMatrix_figs/IM-vs-EM-clust.eps}
		\caption{The relative runtime of FlashR in memory versus on SSDs
		on a dataset with $n=100M$ while varying $p$ (the number of features)
		on the left and varying $k$ (the number of clusters) on the right.}
		\label{perf:stat}
	\end{center}
  \vspace{-15pt}
\end{figure}

As the number of features or clusters increases, the performance gap between
in-memory and external-memory execution narrows and the external-memory
performance approaches in-memory performance for correlation and k-means
but not Naive Bayes (Figure \ref{perf:stat}). This observation conforms with
the computation and I/O complexity of the algorithms in Table \ref{tbl:algs}.
For correlation and k-means, the number of clusters or features causes computation
to grow more quickly than the I/O, driving performance toward a compute bound.
% When the number of features
%gets larger, the computation of matrix multiplication in
%correlation and SVD grows more rapidly than I/O and eventually CPU becomes
%the bottleneck. The current implementation of correlation requires an additional
%pass on the input matrix to compute column-wise mean values, which results in
%lower external-memory performance. Similarly, as the number of clusters
%increases, the computation of k-means and GMM increases rapidly and
%these algorithms are dominated by their CPU computation as the number
%of clusters gets larger. 
The compute bound can be realized on few features or clusters for an I/O throughput of 10GB/s.
Because most of the machine learning algorithms in Table \ref{tbl:algs} has
computation complexity grow quadratically on $p$, we expect FlashR on SSDs to
achieve the same speed as in memory on datasets with a higher dimension size.

%\begin{figure}
%	\begin{center}
%		\footnotesize
%		\includegraphics{FlashMatrix_figs/IM-vs-EM-clust.eps}
%		\caption{{\em Clustering} scaleup: FlashMatrix performance on SSDs 
%      normalized to in-memory performance as the number of clusters vary. 
%%      KMeans operates on the \rb{Friendster32} matrix and GMM on the  
%%			by its performance in memory. As the number of clusters increases,
%%			the external-memory performance of these implementations approach
%%			to their in-memory performance.}
%}
%		\label{perf:clust}
%	\end{center}
%\end{figure}

\begin{comment}
\subsection{Effectiveness of optimizations}
We illustrate the effectiveness of our memory optimizations in FlashR.
We focus on two main optimizations: matrix operation fusion in main memory
to reduce data movement between SSDs and main memory (mem-fuse), and matrix
operation fusion in CPU cache to reduce data movement between main memory and
CPU cache (cache-fuse).

Both optimizations have significant performance improvement on all
algorithms when FlashR runs on SSDs (Figure \ref{perf:em_opts}).
Operation fusion in main memory (mem-fuse) achieves
substantial performance improvement in most algorithms, even in GMM,
which has the highest asymptotic computation complexity. 
%Even though the SSDs deliver an I/O throughput of 10GB/s, 
Materializing every matrix operation
separately causes SSDs to be the main bottleneck in the system and
fusing matrix operations in memory significantly reduces I/O.
% and improves performance by a large factor. 
Operation fusion in the CPU cache (cache-fuse) doubles or even triples
performance in some of the algorithms,
which suggests that memory bandwidth is a limiting performance factor once 
I/O has been optimized.
 %This suggests that with sufficient 
%I/O optimizations, many machine learning algorithms that run on fast SSDs can be bottlenecked by
%the bandwidth of main memory, instead of I/O. 
%Even though it is less noticeable,
%Finally, reducing large memory allocation (mem-alloc) improves I/O performance and almost doubles
%the overall performance of all algorithms.

\begin{figure}
	\begin{center}
		\footnotesize
		\includegraphics{FlashMatrix_figs/opts-EM.eps}
		\vspace{-10pt}
		\caption{The effectiveness of optimizations on different algorithms
		running on SSDs. The optimizations are applied to FlashR
		incrementally.}
		\label{perf:em_opts}
	\end{center}
  \vspace{-15pt}
\end{figure}
\end{comment}
