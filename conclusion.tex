We present FlashR, a matrix-oriented programming framework that executes
R-programmed machine learning algorithms in parallel and out-of-core
automatically. FlashR scales to large datasets by utilizing commodity SSDs.

%For simplicity and generality, the core of FlashR only implements
%a small number of generalized matrix operators (GenOps). It reimplements
%many matrix operations in R \textit{base} package with GenOps to provide
%a familiar programming environment to users. To improve performance,
%FlashR uses vectorized element functions (VEleFuns) to reduce the
%overhead of function calls and fuses matrix operations to reduce data movement
%between CPU and SSDs.

Although R is considered to be slow and unable to scale to large datasets,
we demonstrate that with sufficient system-level optimizations, FlashR powers
the R programming interface to achieve high performance and scalability
for developing many machine learning algorithms. R implementations executed in FlashR
outperform H$_2$O and Spark MLlib on all algorithms by a factor of $3-20$, using
the same shared memory hardware. FlashR scales to datasets with billions of
data points easily with negligible amounts of memory and completes all
algorithms within a reasonable amount of time.

Even though the current I/O technologies, such as solid-state drives (SSDs),
are an order of magnitude slower than DRAM, the external-memory execution
of many algorithms in FlashR achieves performance approaching their in-memory
execution. We demonstrate that an I/O throughput of 10 GB/s saturates the CPU
for many algorithms, even in a large parallel NUMA machine. 

FlashR simplifies the programming effort of writing parallel and out-of-core
implementations for large-scale machine learning. With FlashR, machine learning
researchers can prototype algorithms in a familiar programming environment,
while still getting efficient and scalable implementations.
We believe FlashR provides new opportunities for developing large-scale
machine learning algorithms.
