Basic Linear Algebra Subprograms (BLAS) defines a small set of vector and
matrix operations for shared memory. There exist a few highly-optimized BLAS
implementations, such as MKL \cite{mkl} and ATLAS \cite{atlas}. 
Distributed-memory libraries \cite{trilinos, petsc, elemental}
build on BLAS and distribute computation with MPI.
BLAS provides a limited set of matrix operations, which requires
users to manually parallelize the remaining matrix operations.
%In contrast, FlashR provides a few parallelized GenOps that 
%represent common data access patterns. 
%Each GenOp covers a large number of matrix operations.

Recent works on out-of-core linear algebra \cite{Toledo99, Quintana-Orti12}
redesign algorithms to achieve efficient I/O access and reduce I/O
complexity. These works are orthogonal to our work and can be adopted.
Optimizing I/O
alone is insufficient. To achieve performance comparable to state-of-the-art
in-memory implementations, it is essential to move data efficiently both from
SSDs to memory and from memory to CPU caches.

MapReduce \cite{mapreduce} has been used for parallelizing machine learning
algorithms \cite{Chu06}. Even though MapReduce simplifies parallel programming,
it still requires low-level programming.
As such, frameworks are built on top of MapReduce to reduce programming complexity.
For example, SystemML \cite{systemml} develops an R-like script language for
machine learning. MapReduce is inefficient for matrix operations because
its I/O streaming primitives do not match matrix data access patterns.

The Spark \cite{spark} is a distributed, in-memory framework that provides more
primitives for efficient computation and provides a highly-optimized machine
learning library (MLlib, \cite{mllib}).
We compare FlashR with MLlib. Spark is the most efficient distributed engine.

Efforts to parallelize array programming include
Revolution R \cite{rro} and Matlab's parallel computing toolbox, which
offer multicore parallelism and explicit distributed parallelism using MPI and MapReduce. 
Other works focus on implicit parallelization.
Presto \cite{presto} extends R to sparse matrix operations in distributed memory for graph
analysis. Ching et. al \cite{Ching12} parallelize APL code by
compiling it to C. Accelerator \cite{accelerator} compiles
data-parallel operations on the fly to execute programs on a GPU.

OptiML \cite{optiml} is a domain-specific language for developing machine
learning in a heterogeneous computation environment such as multi-core
processors and GPU. It designs a new programming language and relies on
a compiler to generate code for the heterogenous environment.

