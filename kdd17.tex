\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix}

\pdfoutput=1

\usepackage{booktabs} % For formal tables
\usepackage{epsfig,endnotes}
\usepackage{comment}
%\usepackage{subfigure}
\usepackage{url}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{minted}
\usepackage{bm}
\usepackage{paralist}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{color,soul}
\usepackage{authblk}
%\usepackage{multirow}
\usepackage{float}
%\usepackage[pdftex]{color}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}

\newcommand{\dz}[1]{{\color{blue}{\it DZ: #1}}}
\newcommand{\rb}[1]{{\color{red}{\it RB: #1}}}

\newcommand{\para}[1]{\vspace{5pt}\noindent\textbf{{#1:}}}


\begin{document}
\title{FlashR: Parallelize and Scale R for Machine Learning}
%\titlenote{}
%\subtitle{}
%\subtitlenote{}

\author[1]{\rm Da Zheng}
\author[1]{\rm Disa Mhembere}
\author[3]{\rm Joshua T. Vogelstein}
\author[2]{\rm Carey E. Priebe}
\author[1]{\rm Randal Burns}
\affil[1]{Department of Computer Science, Johns Hopkins University}
\affil[2]{Department of Applied Mathematics and Statistics, Johns Hopkins University}
\affil[3]{Department of Biomedical Engineering, Johns Hopkins University}

\maketitle

\begin{abstract}
R is one of the most popular programming languages for statistics and machine
learning, but the R framework is relatively slow and unable to scale to large
datasets. The general approach for speeding up an implementation in R is to
implement the algorithms in C or FORTRAN and provide an R wrapper. FlashR takes
a different approach: it executes R code in parallel and scales the code beyond
memory capacity by utilizing solid-state drives (SSDs) automatically. It
provides a small number of generalized 
operations (GenOps) upon which we reimplement a large number of
matrix functions in the R \textit{base} package. As such, FlashR parallelizes
and scales existing R code with little/no modification. To reduce data movement
between CPU and SSDs, FlashR evaluates matrix operations lazily, fuses
operations at runtime, and uses cache-aware, two-level matrix partitioning.
We evaluate FlashR on a variety of machine learning and statistics algorithms 
on inputs of up to four billion data points.
FlashR out-of-core tracks closely the performance of FlashR in-memory.
The R code for machine learning algorithms executed in FlashR
outperforms the in-memory execution of H2O and Spark MLlib by a factor of
$2-10$ and outperforms Revolution R Open by more than an order of magnitude.
\end{abstract}

\section{Introduction}
\input{intro}

\vspace{-10pt}
\section{Related Work}
\input{relwork}

\input{design}

\input{eval}

\section{Conclusions}
\input{conclusion}

{\footnotesize \bibliographystyle{acm}
\bibliography{kdd17}}

\end{document}
