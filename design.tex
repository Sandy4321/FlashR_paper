\section{Design}

FlashMatrix is a general-purpose data analysis framework with a matrix-oriented
programming interface. This work mainly focuses on dense matrices and scales
dense matrix operations beyond memory capacity by utilizing fast I/O devices
such as solid-state drives (SSDs) in a non-uniform memory architecture (NUMA).
FlashMatrix uses R as its main programming interface and executes R code
automatically in parallel and out of core.

Figure \ref{fig:arch} shows the architecture of FlashMatrix. The main programming
interface is a small number of generalized matrix operators (GenOps). As such,
FlashMatrix only focuses on optimizations on these generalized operators
to greatly simplify the implementation and improve the expressiveness of
the framework. The optimizer in FlashMatrix aggressively merges operators to
reduce CPU cache misses and I/O accesses and achieve better parallelization.
FlashMatrix stores large matrices on SSDs through SAFS \cite{safs},
a user-space filesystem for a large SSD array, to fully utilize high I/O
throughput of the SSDs.

\begin{figure}
\centering
\includegraphics[scale=0.3]{./architecture.pdf}
\caption{The architecture of FlashMatrix.}
\label{fig:arch}
\end{figure}

FlashMatrix exposes the GenOps in the R interface and reimplements many functions
in the R \textit{base} package with the GenOps to provide a flexible programming
interface for machine learning algorithms. The functions provided by the R
interface are categorized into seven classes:
\begin{itemize}
	\item The generalized matrix operators.
	\item The construction functions create vectors and matrices of the specified
		shape and fill them with specified data (e.g., sequence number or random
		number).
	\item The conversion functions convert between FlashMatrix matrices and
		R matrices to enable interaction between FlashMatrix and the R framework.
	\item The transformation functions changes the shape of a matrix. The commonly
		used functions include matrix transpose, matrix combination such as
		\textit{rbind} and \textit{cbind} and submatrix extraction.
	\item element-wise computation
	\item aggregation
	\item matrix multiplication.
\end{itemize}

FlashR represents VUDFs symbolically and selects the right VUDF instance for
the input matrices at runtime. If the two input matrices in a binary GenOp have
different element types, FlashR automatically casts one of the matrices to
match with the other. FlashR follows the same convention of type casting and
always cast the element type to a high-rank type.

%\subsection{SAFS}

%SAFS \cite{safs} is a user-space filesystem for a high-speed SSD array in
%a NUMA machine. It is implemented as
%a library and runs in the address space of its application. It is deployed
%on top of the Linux native filesystem. SAFS was originally designed for
%optimizing small I/O accesses. However, FlashMatrix accesses data in matrices
%sequentially and
%generates much fewer but much larger I/O. Therefore, we provide additional
%optimizations to maximize sequential I/O throughput from a large SSD array.

%The first optimization is to enable polling for I/O to reduce thread context
%switches. On a high-speed SSD array, the latency caused by a thread context
%switch becomes noticeable under a sequential I/O workload and it becomes
%critical to avoid thread
%context switch to gain I/O performance. If the computation in application
%threads does not saturate CPU, SAFS will put the application threads into
%sleep while they are waiting for I/O. This results in many thread context
%switches and underutilization of both CPU and SSDs. To saturate I/O,
%an application thread issues asynchronous I/O to SAFS and poll for I/O
%completion after completing all computation available to it. Polling avoids
%a thread from being switched out during I/O access and effectively maximizes
%I/O throughput of a high-speed SSD array.

%To better support access to many files simultaneously, SAFS stripes data in
%a file across SSDs with a different striping order for each file. Due to
%the sequential I/O workload, FlashMatrix stripes data across SSDs with a large
%block size, on the order of megabytes, to increase I/O throughput and reduce
%write amplification on SSDs \cite{Tang15}. Such a large block size may cause
%storage skew for small files on a large SSD array if every file stripes data
%in the same order. Using the same striping order also causes skew in I/O access.
%Therefore, SAFS generates a random striping order for each file to evenly
%distribute I/O among SSDs. SAFS stores the striping order with a file for
%future data retrieval.

%When accessing a file sequentially from SSDs, we maintain a set of memory buffers
%for I/O access to reduce the overhead of memory allocation.
%We use large I/O to increase I/O throughput. As such, we need to allocate
%a large memory buffer for I/O access.
%The operating system usually allocates a large memory buffer with \textit{mmap()}
%and populates the buffer with physical pages when it is used. It is
%computationally expensive to populate
%large memory buffers frequently. When accessing high-throughput I/O devices,
%such overhead can cause substantial performance loss. Therefore, we keep a set
%of memory buffers allocated previously and reuse them for new I/O requests.

\subsection{Dense matrices}
Dense matrices are the main data types in FlashMatrix. A vector is stored
as a one-column dense matrix. In FlashMatrix, a dense matrix can be physically
stored in memory or on SSDs or represented by a sequence of computation.
All matrices in FlashMatrix are immutable due to lazy evaluation.

%A matrix has two identifiers: the \textit{matrix identifier} that indicates
%the matrix itself and the \textit{matrix data identifier} that indicates
%the data in the matrix. When a matrix is cloned or transposed, the new matrix
%has a different \textit{matrix identifier} but shares the same
%\textit{matrix data identifier} with the original matrix.

\subsubsection{Tall-and-skinny matrix} \label{sec:tas_mat}

FlashMatrix optimizes for tall-and-skinny (TAS) dense matrices due to their
common existence in many machine learning tasks. As suggested by the name,
a dimension of these matrices is much larger than the other (Figure \ref{fig:tas_mat}).
In machine learning, many data matrices contain a large number of samples with
a relatively small number of features, so the shape of the data matrices is
usually tall and skinny; if a data matrix has many features, the first step
usually is to reduce dimension to generate a tall-and-skinny matrix. FlashMatrix
specifically optimizes for TAS dense matrices with at tens of columns
and handle wider dense matrices by combining multiple TAS dense matrices
(Section \ref{mat_group}). A vector in FlashMatrix is a special case of a TAS
matrix. This section describes optimizations on TAS matrices and all of
the optimizations can be applied to wide-and-narrow matrices similarly.

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{./dense_matrix.pdf}
	\caption{The format of a tall-and-skinny dense matrix.}
	\label{fig:tas_mat}
\end{figure}

FlashMatrix uses two-level horizontal partitioning on TAS matrices for efficient
data access to SSDs and main memory (Figure \ref{fig:tas_mat}): I/O-level
partitioning and CPU-level partitioning. All elements in an I/O-level partition
are stored contiguously regardless of the data layout in the matrix. Each I/O
access reads the entire I/O-level partition and thus its size determines an I/O
size (usually in the order of megabytes). A worker thread gets an I/O-level
partition from each matrix for
computation and the partition size needs to be relatively large to reduce overhead
of retrieving a partition from a matrix. We further split a matrix into smaller
partitions to reducing CPU cache misses when evaluating a sequence of matrix
operations (Section \ref{sec:lazy_eval}). For this level of partitioning, we keep
a partition small enough (in the order of kilobytes) to fit in CPU L1/L2 cache.
FlashMatrix determines the number of rows in a CPU-level partition, which is
affected by the number of columns in a matrix. The number of rows in a partition
is always $2^i$ except the last partition.

%\begin{itemize}
%\item NUMA-level partitioning: When a TAS
%matrix is stored in memory, it is partitioned across NUMA nodes to achieve
%data locality and fully utilize memory bandwidth. NUMA-level partitioning
%maps partitions of different vectors/matrices involved in computation
%to the same NUMA node to reduce inter-processor communication. As such,
%the partition size (the number of rows in a partition) is a global parameter
%and is not affected by the number of columns in a matrix.
%\item I/O-level partitioning:
% Both in-memory and external-memory matrices have I/O-level partitioning.
%For an in-memory matrix, the partition size determines the size of
%contiguous memory required in memory allocation (see Section \ref{sec:mem}).
%The number of rows in an I/O-level partition is determined locally
%and can be affected by the number of columns in a matrix. \dz{Is this necessary?}
%\end{itemize}

FlashMatrix supports both row-major and column-major matrix layout to avoid
physical data copy for matrix operations such as matrix transpose. Due to
the partitioning scheme shown above, a column in a column-major TAS matrix
is not stored contiguously (Figure \ref{fig:tas_mat}). FlashMatrix optimizes
matrix operations for both data layouts.
The layout of an output matrix is generally determined by a GenOp.

\subsubsection{Virtual matrix} \label{virt_mat}
In many cases, we do not need to store the data of a matrix physically. Instead,
we can compute and generate its data on the fly when the matrix is involved in
computation. We refer to such matrices as \textit{virtual matrices}, which
store computation and potentially the reference to some other matrices required
by the computation. A simple example is a matrix with
all elements having the same value. For such a matrix, we only need to store
a single value and construct its matrix partitions during computation.

\textit{Virtual matrices} are essential for lazy evaluation (in Section
\ref{sec:lazy_eval}). All GenOps may output \textit{virtual matrices} that
represent the computation result and only store the computation of the GenOps
and the references to the input matrices of the GenOps. This strategy is
essential for both in-memory and external-memory optimizations to improve
performance. It significantly reduces data access to memory and SSDs and
memory allocation overhead.

%\subsubsection{Cached matrix}
%Memory cache is necessary for external-memory matrices on a machine with
%substantial memory.
%Even though SSDs have significantly improved I/O performance, they are still
%an order of magnitude slower than DRAM. Unfortunately, we cannot rely on
%the page cache in SAFS \cite{sa-cache} to buffer some portion of a dense matrix
%because streaming a matrix to memory always evicts existing data in the page
%cache and generates zero cache hits. Therefore, we explicitly cache some portion
%of a dense matrix.

%We store a matrix cache as an in-memory dense matrix.
%To effectively cache data in a dense matrix, we store a tall matrix in
%column-major and cache the first few columns; we store a wide matrix in row-major
%and cache the first few rows. As such, when computation requests an I/O-level
%partition of a matrix, we only need to issue a single I/O request to read
%the remaining columns or rows to reconstruct the I/O-level partition.

%We use a write-through policy for the matrix cache. As such, even when part of
%a dense matrix is cached, we keep a complete copy of the dense matrix on SSDs.
%The benefit of a write-through policy is to overlap computation and I/O when
%a dense matrix is created and avoid I/O latency when removing the cache.

\subsubsection{A group of dense matrices} \label{sec:mat_group}
FlashMatrix represents a tall matrix with many columns with a group of
tall-and-skinny matrices and a wide matrix with a group of short-and-wide
matrices. We denote a group of tall-and-skinny matrices with \textit{tall
matrix group} and a group of short-and-wide matrices with \textit{wide matrix
group}. We construct a special \textit{virtual matrix} to represent
the group of dense matrices and decomposes a matrix operation on the combined
matrix into multiple matrix operations on individual matrices in the group to
take advantage of the optimizations on matrix operations on TAS matrices.
This strategy resembles 2D-partitioning on a dense matrix.

%\begin{figure}
%	\centering
%	\includegraphics[scale=0.5]{./matrix_group.pdf}
%	\caption{A group of matrices to form a tall matrix with many columns (a)
%	and a wide matrix with many rows (b).}
%	\label{fig:mat_group}
%\end{figure}

%A group of matrices should be stored in a single SAFS file to reduce the number
%of SAFS files.

%append an element to a vector can be implemented as physically appending the element
%to the vector. The result vector becomes the new vector, and the original vector
%becomes the sub-vector of the new vector.

\subsection{Generalized computation operations} \label{sec:genop}
To enhance flexibility and simplify the implementation, FlashMatrix only
provides four generalized operators (GenOps) on matrices: \textit{inner product},
\textit{apply}, \textit{aggregation} and \textit{groupby}. Each of the operators
represents a data access pattern and accepts user-defined functions (UDF) to
perform actual computation. GenOps accept UDFs at runtime to support dynamic
programming languages such as R. Therefore, we have to pass a pointer of a UDF
to a GenOp.

\textit{Inner product} is a generalized matrix multiplication. It replaces
multiplication and addition in matrix multiplication with two UDFs,
respectivel, and the second UDF performs aggregation.
We can define many operations with inner
product. For example, we can use inner product to compute various pair-wise
distance matrics of vectors such as Euclidean distance \cite{euclidean} and
Hamming distance \cite{hamming}. For dense matrices, we mainly focus on
optimizing two cases: inner product of a wide matrix and a tall matrix (denoted
with \textit{inner\_prod\_wide}) and inner product of a tall matrix and a small
matrix (denoted with \textit{inner\_prod\_tall}). It is usually impractical to
materialize inner product of a large tall matrix and a large wide matrix due to
the large computation and space complexity. As such, we rely on users to
transform this form of inner product to the other two forms to reduce computation
and space. Even though we can implement matrix multiplication with inner product,
FlashMatrix relies on BLAS to implement matrix multiplicatoin for
float-point matrices to achieve speed and precision required by
many numeric libraries such as the Trilinos eigensolver \cite{anasazi, FlashEigen}.

\textit{Apply} is a generalized form of element-wise operations and has
multiple variants. The simplest form denoted with \textit{sapply} is
a generalized element-wise unary operation whose UDF takes one element at a time
in a matrix and outputs one value. We can use it to implement many unary
operations such as negation, square root or element type casting
on a matrix. The second form denoted with \textit{mapply2} is a generalized
element-wise binary operation whose UDF takes one element from each
matrix and outputs a single value. We use it to implement many binary
matrix operations such as matrix addition and subtraction. The third form
(denoted with \textit{mapply\_row}) and the fourth form (denoted with
\textit{mapply\_col}) are generalized
operations that apply the input vector to every row or column of the input
matrix and outputs a matrix with the same shape as the input matrix. Each time
a UDF is applied to to an element from a row or a column of the input matrix
and an element from the vector and outputs a single value. Although
\textit{mapply\_row} and \textit{mapply\_col} can also be implemented by
replicating the input vector to create a matrix and applying \textit{mapply2}
on the input matrix and the created matrix, \textit{mapply\_row} and
\textit{mapply\_col} avoid CPU cache polution to improve performance.

\textit{Aggregation} takes multiple elements and a aggregation UDF and outputs
a single element. It has two forms on a matrix.
The first form denoted by \textit{agg} aggregates over all elements on a matrix.
Matrix summation is an example of the first form. The second form denoted by
\textit{agg\_row} and \textit{agg\_col} computes aggregation over each individual
row or column. Row sum and column sum are examples of the second form.
To enable parallelization, the user-defined aggregation operation needs to
provide an aggregation function that runs on the elements of the input matrix
and a combination function that runs on the partially aggregated
results. For many aggregation operations such as summation, the aggregation
and combination functions are the same.

\textit{Groupby} on a matrix groups rows (\textit{groupby\_row}) or columns
(\textit{groupby\_col}) of a matrix based on a vector of categorical values
and invokes aggregation UDF on the rows or
columns associated with the same categorical value. Matrix \textit{groupby}
can be used in many classification and clustering algorithms that compute
aggregation on the data points in a class or in a cluster. Aggregation UDFs
in \textit{groupby} are the same
to the ones in \textit{aggregation} and each requires an aggregation function
and a combine function to enable parallelization.

\subsection{Vectorized user-defined function}
All of the GenOps take vectorized UDFs (VUDFs) \dz{is this a new concept?},
which operates on a vector of elements instead of an individual element.
The GenOps in FlashMatrix take the pointers to UDFs as input at runtime.
As such, there is significant computation overhead due to a tremendous number
of function calls if we apply UDF to every individual element in a matrix.
In contrast, VUDFs operate on a vector of elements to amortize the overhead
of function calls.

We balance the amortization of the overhead of function calls and CPU cache
misses. Because VUDFs operate on vectors, we can no longer keep the input data
of a VUDF in CPU registers. To reduce latency of accessing data in VUDFs,
the input data has to be small enough to be kept in the CPU L1 cache. On the
other hand, more input data in an invocation of VUDF amortizes the overhead of
function calls more aggressively. We choose 128 as the maximum length of
the input vector of a VUDF.
Futher increasing the length does not have noticeable performance improvement.
\dz{I need to show this in the experiment.}

We support three types of VUDFs to support all of the GenOps in Section
\ref{sec:genop}. A VUDF type may have multiple forms.
\begin{itemize}
	\item A \textit{unary} VUDF (denoted with \textit{uVUDF}) has only one form,
		which takes a vector as input and outputs a vector of the same length.
	\item A \textit{binary} VUDF has three forms: the first form (denoted with
		\textit{bVUDF1}) takes two vectors of the same length and outputs
		a vector of the same length as the input vectors; the second form (denoted
		with \textit{bVUDF2}) takes a vector as the left argument and a scalar
		as the right argument and outputs a vector with the same length as
		the input vector; the third form (denoted with bVUDF3) takes a scalar
		as the left argument and a vector as the right
		argument and outputs a vector. The reason of having both the second and third
		forms is to support non-commutative binary operations such as division and
		subtraction.
	\item An \textit{aggregation} VUDF is composed of two functions:
		\textit{aggregate} and \textit{combine}. For many aggregation VUDFs
		such as summation, \textit{aggregate} and \textit{combine} are the same.
		However, for some aggregation such as \textit{count}, \textit{aggregate}
		and \textit{combine} are different. Both functions may have two forms:
		the first one (denoted with \textit{aVUDF1}) takes a vector and outputs
		a scalar; the second one (denoted with \textit{aVUDF2}) takes two
		vectors of the same length and outputs a vector. For many aggregation
		operations in math, such as summation and product, the \textit{aggregate}
		and \textit{combine} functions are the same and have both \textit{aVUDF1}
		and \textit{aVUDF2} forms.
\end{itemize}

FlashMatrix implements many commonly used VUDFs by default. These VUDFs wrap
basic operations built in many programming languages and libraries. For example,
FlashMatrix provides arithmetic operations such as \textit{addition} and
\textit{Psubtraction}, relational operations such as \textit{equal to} and
\textit{less than}, logical operations such as \textit{logical AND} and
\textit{logical OR}, as well as commonly used math functions such as computing
absolute values and square root. FlashMatrix also provides a set of VUDFs to
cast primitive element types.

For each basic operation wrapped in a VUDF, FlashMatrix provides multiple
VUDF implementations to support different element types. To reduce the number
of binary VUDF implementations,
FlashMatrix only provides the implementations that take two input arguments of
the same type. If a GenOp gets two matrices with different
element types, it first casts the the element type of one matrix to match
the other. Type casting follows the usual arithmetic conversions \cite{}
commonly seen in many programming languages. The type casting operation is
implemented with \textit{sapply} and is performed laziy (in Section
\ref{sec:lazy_eval}).

We use CPU vector instructions such as AVX \cite{avx} to accelerate
the computation in a VUDF. The current implementation of FlashMatrix heavily
relies on auto-vectorization
of a compiler such as GCC to vectorize computaion, but it provides hints and
transforms code to help auto-vectorization. For example, a VUDF in FlashMatrix
frequently operates on vectors with data well aligned in memory and of
the length defined at compile time, so we inform the compiler of the data alignment
and the vector length. Some compilers do not automatically vectorize
aggregation operations well. We manually create a small vector of reduction
variables, flatten the loop and transform the original aggregation operation
into aggregation onto the vector of reduction variables.

\subsection{Implementation of GenOps with VUDF}

FlashMatrix selects the right form of a VUDF for a given GenOp based on the data
layout and the shape of the input matrices. The selection criteria is to maximize
the amortization of the overhead of invoking VUDFs.

Some GenOps can be applied to matrices with any data layout and any shape
efficiently. For example, \textit{sapply} and \textit{mapply2} only requires
the input matrices and the output matrix to have the same data layout. For tall
column-major matrices and wide row-major matrices,
each partition has long columns and long rows, respectively. For tall row-major
matrices and wide column-major matrices, all rows and columns in a partition
is stored in a single piece of memory. As such, we can always pass long vectors
to the VUDF. A similar strategy is applicable to \textit{agg}.

To reduce the overhead of invoking VUDFs, many of the GenOps favor
the column-major order for a tall-and-skinny matrix and the row-major order
for a short-and-wide matrix. These data layouts increase
the length of a vector passed to a VUDF to amortize the function call overhead.
%As such, FlashMatrix can always partition the input
%matrices and feed VUDFs with vectors of the compile-time known length.
Furthermore, for
a tall matrix, the column-major order ensures data in each column in a partition
to be well aligned in memory owing to the partition size, regardless of
the number of columns in the matrix; the similar effect is applied to a wide
matrix. Memory alignment helps CPU vectorization to improve performance.
Before invoking VUDFs on CPU-level partitions, a GenOp such as inner product
may convert the data layout of a CPU-level partition to the preferred data
layout if an input matrix does not have the preferred data layout.
Because a CPU-level partition can fit in the CPU cache, data layout conversion
does not increase data access to main memory. 

When a GenOp applies a VUDF on rows
\textit{mapply\_col} and \textit{mapply\_row} favors
the column-major order for a tall-and-skinny matrix and the row-major order
for a short-and-wide matrix. For example, when applying \textit{mapply\_col}
on a tall column-major matrix, FlashMatrix chooses the \textit{bVUDF1} form to
perform computation on each column of the matrix and the vector; when applying
\textit{mapply\_row} on a tall column-major matrix, FlashMatrix choose
the \textit{bVUDF2} form.
Both \textit{agg\_row}
and \textit{agg\_col}, on the other hand, work better on the preferred data
layout if the aggregation VUDF has the same \textit{aggregate} and \textit{combine}
function with both \textit{aVUDF1} and \textit{aVUDF2} forms. For these aggregation
VUDFs, we can transform \textit{agg\_row} on a column-major matrix to applying
\textit{aVUDF2} to columns of a partition. Similar transformation is applied
to \textit{agg\_col} on a row-major matrix.

Inner product chooses different forms of VUDFs for input matrices with different
shapes. The first UDF of inner product is a binary UDF and the second one is an
aggregation UDF. For the sake of efficiency, inner product requires the second
UDF to have the same \textit{aggregate} and \textit{combine} function with both
\textit{aVUDF1} and \textit{aVUDF2} forms.
\textit{inner\_prod\_tall} uses the \textit{bVUDF2} form of the first
VUDF to computes the outer product of a column from the left matrix and a row
from the right matrix and uses the \textit{bVUDF1} of the second VUDF to output
the final result. Because \textit{inner\_prod\_tall} operates on a CPU-level
partition, all intermediate results are kept in CPU cache and thus we only need
to read the input partition from main memory and write the final result for
the input partitions to main memory. \textit{inner\_prod\_wide} invokes the
\textit{bVUDF1} form of the first VUDF on a row from the left matrix and a column
from the right matrix and invokes the \textit{aVUDF1} form of the second VUDF
on the output of the first VUDF to compute an element of the output from
the input partitions.

Unlike other GenOps, \textit{groupby\_row} and \textit{groupby\_col} are the only
GenOps that do not favor the column-major order for a tall-and-skinny matrix and
the row-major order for a short-and-wide matrix. \textit{groupby\_row} sorts
rows in a partition based on the categorical values associated with each row
and apply the aggregation VUDF on rows directly.

\subsection{Implementation of GenOps on a group of matrices}

When applying a GenOp on a group of matrices,
we decompose the computation into multiple GenOps and apply them to individual
matrices in the group if the GenOp supports decomposition. Decomposing computation
to individual matrices reduces memory copy and increases CPU cache hits and thus
can significantly improve performance. For the GenOps that cannot be decomposed,
we combine the individual matrices on the fly and apply the GenOps on the combined
matrix directly.

We can apply some of the GenOps to individual matrices directly. \textit{sapply}
and \textit{agg} run on individual matrices directly regardless of the shape
and data layout of individual matrices. For other GenOps, we may also apply
them to individual matrices directly without transformation. For a
\textit{tall matrix group}, we can apply \textit{mapply\_col} and \textit{agg\_col}
to individual matrices directly.
Similarly, we can apply \textit{mapply\_row} and \textit{agg\_row} to
individual matrices directly for a \textit{wide matrix group}. \textit{mapply2}
requires the matrices in the two input matrix groups to have the same shape.
\textit{Groupby\_row} on a \textit{tall matrix group} can be decomposed and
applied to individual matrices; \textit{groupby\_col} on a \textit{wide matrix group}
can be decomposed in a similar fashion.

Applying other GenOps to a matrix group requires transformation. If an aggregation
operation provides a combine function, applying \textit{agg\_row} to a group of
tall matrices is transformed into two steps: apply the aggregation function on
each row of individual matrices and apply the combine function on aggregation
results. Similarly, the same strategy is used for \textit{agg\_col} on a group
of wide matrices. When applying \textit{mapply\_row} to a group of tall matrices,
we break the vector into parts to match the number of columns in the individual
matrices in the group. Similarly, we break the vector into parts to match the number
of rows in the individual matrices for \textit{mapply\_col}.

FlashMatrix decomposes inner product on a matrix group in favor of minimizing
the amount of data written to SSDs (Figure \ref{fig:inner_prod}).
For \textit{inner\_prod\_tall}, we first partition the right matrix vertically
so that the inner product of the left matrix and a vertical partition outputs
part of a final output matrix. The number of vertical partitions determines
the number of runs required to complete the inner product on the group of matrices.
If the left matrix is stored on SSDs, the number of vertical partitions on
the right matrix determines the amount of data read from SSDs. The vertical
partition size determines the output matrix size in each run and affect
the memory size. As such, we need to select the vertical partition size of
the right matrix to balance I/O and memory consumption. We horizontally partition
each vertical partition of the right matrix to further decompose the inner
product. We construct a directed acyclic graph (DAG) to evalute the inner
product lazily (Section \ref{sec:lazy_eval}). Similarly, we partition the right
matrix and make a similar choice to balance I/O and memory consumption for
\textit{inner\_prod\_wide}. We also construct a DAG and lazily evaluate
the computation.

\begin{figure}
\centering
\includegraphics[scale=0.4]{./inner_prod_tall.pdf}
\vspace{-5pt}
\caption{}
\vspace{-5pt}
\label{fig:inner_prod}
\end{figure}


\subsection{Lazy evaluation} \label{sec:lazy_eval}
FlashMatrix evaluates matrix operations lazily because it is expensive to
perform each matrix operation individually when matrices become large.
Evaluating matrix operations separately causes significant amount of data
movement for both in-memory and external-memory matrices and considerable
memory allocation overhead for in-memory matrices. Lazy evaluation allows
to fuse matrix operations to minimize data movement as well as improve
parallelization \cite{Ching12}.

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{./DAG.pdf}
	\caption{An operation tree.}
	\label{fig:DAG}
\end{figure}

FlashMatrix constructs a directed acyclic graph (DAG) \cite{} to represent
computation formed by a sequence of matrix operations evaluated lazily
(Figure \ref{fig:DAG}). A lazily evaluated GenOp outputs a \textit{virtual matrix}
to capture the matrix computation and the input matrices. As such, a computation
DAG is formed with a set of \textit{virtual matrix} nodes (shown as rectangles)
and computation nodes (shown as cycles).
The computation nodes may contain some immutable computation state such as
scalar variables and small matrices involved in the matrix computation.
A computation node may take more than two matrices as input and always outputs
one matrix. An example is inner product on a group of matrices (Figure
\ref{fig:inner_prod}). A matrix node can also be used by multiple computation
node as an input matrix.
To simplify computation evaluation and data flow of a DAG (Section
\ref{sec:materialize}), FlashMatrix requires all \textit{virtual matrices} in
the DAG has the same \textit{long dimension} and the same size in the dimension
(the \textit{long dimension} refers to the dimension of the larger size than
the other).

FlashMatrix allows lazy evaluation on all GenOps but with different policies.
FlashMatrix always lazily evaluate the GenOps that output matrices with
the same \textit{long dimension} size because they can be easily connected
with other nodes in a DAG. \textit{sapply} and \textit{mapply2} always
output a matrix with the same dimension size as the input matrices. Others such
as \textit{agg\_row} on a tall matrix and \textit{agg\_col} on a wide matrix output
a matrix with different shape, but its \textit{long dimension} has the same size
as the input matrix. By default, FlashMatrix evaluates the GenOps that output
matrices with different \textit{long dimensions} or different \textit{long dimension}
sizes. FlashMatrix also supports lazy evaluation on these GenOps and we refer to
the matrices output by these GenOps as \textit{sink matrices}.
FlashMatrix allows \textit{sink matrices} to be connected a DAG but any computation
that uses a \textit{sink matrix} as input cannot be connected to the same DAG.

To enable lazy evaluation, all matrices in FlashMatrix are immutable.
As such, \textit{virtual matrices} can generate the same result every time
they are materialized. As such, every matrix operation generates a new matrix
and a matrix is garbage collected when there are not any references to it.

\subsection{Matrix materialization} \label{sec:materialize}
Lazy evaluation postpones the actual computation of matrix operations, but we
eventually have to materialize the matrix operations. Matrix materialization
is usually triggered when a GenOp that outputs matrices with different
\textit{long dimensions} or different \textit{long dimension} sizes is met.

FlashMatrix allows to materialize any \textit{virtual matrix} in a DAG.
Typically, only the \textit{sink matrices} in a DAG are materialized to reduce
I/O; the TAS \textit{virtual matrices} are materialized on the fly. However,
we need to save the materialized TAS matrices in memory or on SSDs as well in
many iterative algorithms to avoid redundant computation and reduce I/O. As such,
FlashMatrix allows users to set a flag on a TAS \textit{virtual matrix} to
inform FlashMatrix to save materialized results during computation.
\dz{FlashMatrix can materialize a DAG from multiple virtual matrices.}

We partition matrices on a DAG in the \textit{long dimension} for materialization
and parallelization. All TAS matrices (we refer to wide-and-short matrices as TAS
matrices as well) on a DAG have the same \textit{long dimension} and the same size
in the dimension. To simplify materialization, all TAS matrices on a DAG share
the same partition size in the \textit{long dimension}. As such, the partitions
in a DAG are materialized independantly. Owing to the storage scheme for
in-memory matrices, the data of all in-memory matrices in a partition are stored
on the same NUMA node to minimize remote memory access. By default, FlashMatrix
discards data in a partition immediately to reduce memory allocation and I/O
once the data is used by the children matrices.

Materialization of a \textit{sink matrix} is slightly different from a TAS matrix.
A \textit{sink matrix} in FlashMatrix is the output of a form of aggregation
operation on TAS matrices. As such, each thread computes partial aggregation
result from partitions of TAS matrices assigned to the thread and stores partial
aggregation in a local memory buffer. Afterwards, FlashMatrix combines
the partial aggregation results to compute the final result.

%How is a DAG traversed during materialization?
%When materializing a partition on a \textit{virtual matrix}, FlashMatrix requires
%all input partitions have been materialized or read from SSDs.

FlashMatrix uses two levels of partitioning when materializing computation.
It assigns I/O-level partitions (Section \ref{sec:tas_mat}) to a thread as
computation tasks for parallelization. We choose a relatively small partition
size to balance the overhead of accessing a partition, parallelization skew
and memory consumption. A thread further splits an I/O-level partition into
CPU-level partitions and materializes one CPU-level partition at a time.
When a CPU-level partition is materialized, it is passed to the subsequent
operation in the DAG. A CPU-level partition is sufficiently small to fit in
the CPU cache to reduce CPU cache misses when materializing computation in
a DAG. In a DAG, a matrix may be required by multiple GenOps. As such,
a TAS matrix always buffers one materialized CPU-level partition in each
thread. \dz{We need to identify transpose of a matrix.}

FlashMatrix uses a global task scheduler to assign tasks to threads dynamically
for load balancing and I/O merging. However, SSDs require large
writes to achieve sustainable write throughout and reduce write amplification
\cite{}. As such, FlashMatrix delays writing the materialized partitions to
SSDs and merge multiple materialized partitions into a single write because
the global task scheduler assigns consecutive partitions to threads.

%To keep data in CPU cache as long as possible, we reuse the memory buffers
%to reduce the number of memory buffers used in the computation and avoid CPU
%cache polution.

%\subsection{Memory management} \label{sec:mem}
%FlashMatrix manages large memory allocation to reduce memory allocation overhead.
%It is expensive to allocate large memory. Linux uses \textit{mmap} to allocate
%large memory in the order of megabytes. Linux relies on page fault to populate 
%the memory allocated by \textit{mmap} with physical pages when they are used
%for the first time. The overhead becomes considerable when we allocate and
%deallocate large memory constantly. Frequent page faults prevent computation
%from fully utilizing CPUs in a large parallel machine. FlashMatrix potentially
%requires more memory allocation because all data containers are immutable due
%to lazy evaluation and each GenOp outputs a new matrix.

%\begin{figure}
%	\centering
%	\includegraphics[scale=0.5]{./matrix_mem.pdf}
%	\caption{Memory layout of a tall-and-skinny dense matrix.}
%	\label{fig:mat_mem}
%\end{figure}

%To reduce the overhead of memory allocation, we recycle memory allocated for
%in-memory matrices. Instead of deallocating memory when a matrix is destroyed,
%we retain the memory and assign them to the next matrix. To help recycle memory,
%we allocate fixed-size memory chunks for matrices. The size of a memory chunk
%is a global parameter and is the same for all 
%matrices. As such, memory chunks allocated for a matrix can be used by another
%matrix with a totally different shape. We perform matrix operations on
%I/O-level partitions and only require an I/O-level partition to be stored in
%contiguous memory. Therefore, we can store a matrix in memory chunks,
%as long as a fixed-size memory chunk is sufficient
%to store an I/O-level partition. In practice, the memory chunk size may
%not be divisible by a I/O-level partition size. Therefore, the memory chunk
%size needs to be much larger than the I/O-level partition size to increase
%memory utilization.

%Similarly, we maintain per-thread memory buffer pools to store data read from
%SSDs. These memory buffers need to be
%the same size as I/O-level partitions, which is in the order of megabytes
%to maximize I/O throughput of an SSD. Due to asynchronous I/O, a worker thread
%needs to allocate a small number of memory buffers for each matrix
%during computation. Because all partitions in a vector or a matrix have
%the same size, the memory buffers are reused for processing other partitions
%of vectors or matrices.

\subsection{Data placement}
FlashMatrix by default keeps large matrices on SSDs.

If the output matrix of a GenOp has a different \textit{long dimension} from
the input matrices, the output matrix is considered small and is by default
materialized in memory. For example, \textit{agg\_row} on a wide matrix and
\textit{agg\_col} on a tall matrix outputs a vector, which is also considered
small and is by default kept in memory; the output matrix of the inner product
of a wide matrix with a tall matrix is also considered small and is always
kept in memory.

If the output matrix of a GenOp has the same \textit{long dimension} as
the input matrices, it is not materialized by default and thus does not require
memory storage. These matrices are materialized on the fly and users can inform
FlashMatrix to save the materialized data in the specified memory storage.
