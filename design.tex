\section{Design}

FlashMatrix is a matrix-oriented programming framework for general data analysis.
This work mainly focuses on dense matrices and scales dense matrix operations
beyond memory capacity by utilizing fast I/O devices, such as solid-state drives
(SSDs), in a non-uniform memory architecture (NUMA).

Figure \ref{fig:arch} shows the architecture of FlashMatrix. 
A small number of generalized matrix operators (GenOps)
 simplify the implementation and improve expressiveness of
the framework. The optimizer aggressively merges operations to
reduce data movement in the memory hierarchy and achieve better parallelization.
It stores matrices on SSDs through SAFS \cite{safs},
a user-space filesystem for SSD arrays, to fully utilize high I/O
throughput of SSDs and deploys a set of I/O optimizations.

\begin{figure}
\centering
\includegraphics[scale=0.3]{FlashMatrix_figs/architecture.pdf}
\caption{The architecture of FlashMatrix}
\label{fig:arch}
\end{figure}

\subsection{Programming interface}

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|l|l|l|}
\hline
GenOp & Description \\
\hline
$C=sapply(A, f)$ & $C_{i,j}=f(A_{i,j})$ \\
\hline
$C=mapply(A, B, f)$ & $C_{i,j}=f(A_{i,j}, B_{i,j})$ \\
\hline
$C=mapply.row(A, B, f)$ & $C_{i,j}=f(A_{i,j}, B_j)$ \\
\hline
$C=mapply.col(A, B, f)$ & $C_{i,j}=f(A_{i,j}, B_i)$ \\
\hline
$c=agg(A, f)$ & $c=f(A_{i,j}, c)$, over all $i$, $j$ \\
\hline
$C=agg.row(A, f)$ & $C_i=f(A_{i,j}, C_i)$, over all $j$ \\
\hline
$C=agg.col(A, f)$ & $C_j=f(A_{i,j}, C_j)$, over all $i$ \\
\hline
%$C=groupby(A, f)$ & $C_k=f(A_{i,j}, C_k)$, where $k$ \\
$C=groupby.row(A, B, f)$ & $C_{k,j}=f(A_{i,j}, C_{k,j})$,\\ & where $B_i=k$, over all $i$ \\
\hline
$C=groupby.col(A, B, f)$ & $C_{i,k}=f(A_{i,j}, C_{i,k})$,\\ & where $B_j=k$, over all $j$ \\
\hline
$C=inner.prod(A, B, f1, f2)$ & $t=f1(A_{i,k}, B_{k,j})$,
\\ & $C_{i,j}=f2(t, C_{i,j})$, over all $k$ \\
\hline
\end{tabular}
\normalsize
\end{center}
\caption{Generalized operators (GenOps) in FlashMatrix.
$A$, $B$ and $C$ are matrices, and $c$ is a scalar.}
\label{tbl:genops}
\end{table}

FlashMatrix provides a matrix-oriented functional programming interface.
Table \ref{tbl:genops} lists all GenOps, each of which takes matrices and
some functions as input and output new matrices that store computation results.
The input function defines computation on individual elements in input matrices.
The GenOps can be classified into four categories: (a) \textit{apply} for
element-wise operations, (b) \textit{agg} for aggregation on a matrix
or on rows/columns, (c) \textit{groupby} for splitting rows/columns
into groups and computing aggregates in each group,
(d) \textit{inner product} for the inner product of two matrices.

The example in Figure \ref{fig:kmeans} computes an iteration of k-means
\cite{kmeans} using GenOps. It first uses \textit{fm.inner.prod} to
compute the Euclidean distance between every data point and every cluster center
and outputs a matrix with each row representing the distances to centers.  
It uses \textit{fm.agg.row} to find the closest
cluster for each data point.  The output matrix 
assigns data points to clusters. It then uses \textit{fm.groupby.row} to count
the number of data points in each cluster and compute the mean of each cluster.

\begin{figure}
\centering
	\footnotesize
	\begin{subfigure}{.25\textwidth}
	\centering
	\begin{minted}[mathescape,
	fontsize=\scriptsize,
	frame=single,
	tabsize=2,
	]{R}
# X is the data matrix.
# C is cluster centers.
kmeans.iter <- function(X,C) {
	# Compute pair-wise distance
	# between data points and
	# centers.
	D<-inner.prod(X,t(C),
			"euclidean","+")
	# Find the closest center.
	I<-agg.row(D,"which.min")
	# Count the number of data
	# points in each cluster.
	one<-rep.int(1,nrow(I))
	CNT<-groupby.row(one,I,"+")
	# Compute the new centers.
	C<-groupby.row(X,I,"+")
	C<-mapply.row(C,CNT,"/")
	list(C=C,I=I)
}
	\end{minted}
	\label{fig:code}
		\caption{R code \rb{this looks bad.}}
	\hspace{20pt}
	\end{subfigure}%
	\begin{subfigure}{.25\textwidth}
	\centering
	\includegraphics[scale=0.5]{FlashMatrix_figs/DAG.pdf}
	\label{fig:dag}
	\caption{DAG}
	\end{subfigure}
	\caption{An example of using GenOps to implement k-means.}
	\label{fig:kmeans}
\end{figure}

\subsection{Dense matrices}
Dense matrices are the main data types in FlashMatrix. A vector is stored
as a one-column dense matrix. A dense matrix can be stored
physically in memory or on SSDs or represented virtually by a sequence of
computations.
For most of data analysis tasks, input data matrices are large and rectangular.
Many of them contain a large number of samples with a relatively
few features \cite{}, while many other contain a large number of features with
a relatively few samples \cite{}. Intermediate matrices and output matrices
from the computation are usually small and may be square. As such, we optimize
dense matrices differently based on their size and shape.
Our treatment describes the data format for tall matrices. 
Wide matrices use similar strategies.

%\subsubsection{Tall-and-skinny matrices} \label{sec:tas_mat}
\noindent \textbf{Tall-and-skinny matrices}:
FlashMatrix optimizes for \textit{tall-and-skinny} (TAS) dense matrices due to their
frequent occurrence in data analysis. Many data matrices contain
a large number of samples with a relatively few features, so data matrices
are tall and skinny; even if the original data matrices have many features,
the first step is usually dimension reduction \cite{} to reduce the number
of features.. FlashMatrix supports both row-major and column-major
layouts (Figure \ref{fig:den_mat} (a) and (b)).  Supporting both formats
allows FlashMatrix to transpose data without a copy. A TAS matrix is stored
as an SAFS file.

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{FlashMatrix_figs/dense_matrix2.pdf}
	\caption{The format of a tall dense matrix.}
	\label{fig:den_mat}
\end{figure}

A TAS matrix is partitioned physically into I/O-partitions (Figure
\ref{fig:den_mat}). We refer to the dimension that are partitioned as
\textit{partition dimension}. All elements in an I/O-partition are stored
contiguously regardless of the data layout in the matrix. All 
I/O-partitions have the same number of rows regardless of
the number of columns in a TAS matrix. The number of rows in
an I/O-partition is always $2^i$. This produces column-major TAS
matrices whose data are well aligned in memory to help CPU vectorization.

%\subsubsection{Block matrices} \label{sec:block_mat}
\noindent \textbf{Block matrices}:
FlashMatrix represents a tall matrix with a group of TAS matrices (Figure
\ref{fig:den_mat} (c)), each with $32$ columns except the last one. We refer
to such a matrix as a \textit{block matrix}. Each of the TAS matrices is
stored as a separate file on SAFS. We decompose a matrix operation
on a group of matrices into operations on individual matrices in the group
to take advantage of the optimizations on TAS matrices.
Coupled with the I/O partitioning on TAS matrices, this strategy enables
2D-partitioning on a dense matrix and each partition fits in main memory.
\dz{Should we explain GenOps on block matrices in more details?}

%\subsubsection{Virtual matrices} \label{virt_mat}
\noindent \textbf{Virtual matrices}:
To support lazy evaluation, FlashMatrix uses \textit{virtual matrices} that
materialize data on the fly during computation and transfer output as input to the
next stage of computation.  The data of a matrix are not stored physically.
All GenOps output virtual matrices. A GenOp on a \textit{block matrix} may outputs
a block \textit{virtual matrix}. Virtual matrices are assembled to construct
a directed acyclic graph (DAG) that represents a sequence of matrix computations.

\noindent \textbf{Sink matrices}: Some of the GenOps, such as aggregation and
groupby, output \textit{virtual
matrices} that have different \textit{partition dimensize} size from
the large input matrices (\dz{a better name?}). We refer to these
\textit{virtual matrices} as \textit{sink matrices}. %The GenOps that
%generate \textit{sink matrices} are shown in Table \ref{tbl:sink}.
%In general, \textit{sink matrices} are small and their materialized results
%are always stored in memory. The maximal size of the output matrix from
%an aggregation is $\sqrt{N}$, where $N$ is the number of elements in the input
%matrix. Although the results of groupby and inner product can potentially be
%large, they are small in practice in most of machine learning and data analyis
%tasks because $k$, the number of classes, and $p1$ and $p2$ are usually much
%smaller than the \textit{partition dimension} of a matrix in these tasks.
In general, sink matrices are small and stored in memory in order to minimize
data written to SSDs. The maximal size of the output matrix from an aggregation
is $\sqrt{N}$ for $N$ elements in the input matrix. The maximal size of
the output matrix from a groupby is $k \times \sqrt{N}$ for $k$ classes.
%For most of machine learning and data analysis tasks, 
The output matrix of the inner product of a wide matrix with a tall matrix is usually small because
the long dimension of these matrices is much larger than the short dimension.

% TODO I think we need more formal definition on sink matrices.
%\begin{table}
%\begin{center}
%\footnotesize
%\begin{tabular}{|l|l|l|}
%\hline
%GenOp & Matrix dim & Output size \\
%\hline
%fm.agg & ($n \times p$) & $1$ \\
%\hline
%fm.agg.row & ($n \times p$), $n < p$ & $n$ \\
%\hline
%fm.agg.col & ($n \times p$), $n > p$ & $p$ \\
%\hline
%fm.groupby.row & ($n \times p$), $n > p$ & $p \times k$ \\
%\hline
%fm.groupby.col & ($n \times p$), $n < p$ & $n \times k$ \\
%\hline
%fm.inner.prod & ($p1 \times n$) $\times$ ($n \times p2$), & $p1 \times p2$ \\
%%			  & $n \gg p1, n \gg p2$ &  \\
%\hline
%\end{tabular}
%\normalsize
%\end{center}
%\caption{GenOps that output \textit{sink matrices}.}
%%\label{tbl:sink}
%\end{table}

%In many cases, we do not need to store the data of a matrix physically. Instead,
%we compute and generate its data on the fly. Such matrices are essential for
%lazy evaluation and we refer to these matrices as \textit{virtual matrices}.

%This strategy is essential to reduce data
%movement in the memory hierarchy and memory allocation overhead for creating
%new matrices.

\subsection{Reducing data movement}
When evaluating a sequence of matrix operations, FlashMatrix lazily evaluates
GenOps and constructs a DAG automatically because most of the GenOps do not
contain enough computation to achieve in-memory performance. FlashMatrix grows
a DAG as large as possible to increase the ratio of computation to I/O.
Then it performs computation in a DAG all together to minimize data movement
in the memory hierarchy.

Figure \ref{fig:kmeans} (b) shows a DAG for the k-means of Figure
\ref{fig:kmeans}. A DAG comprises a set of matrix nodes (rectangles)
and computation nodes (ellipses). The majority of matrix nodes are
virtual matrices (dashed line rectangles).
%, which only contains the corresponding matrix operations and input matrices. 
For k-means, only the input matrix \textit{X} has materialized data.
A computation node references a GenOp and input matrices and
it may contain some immutable computation state, such as scalar variables and
small matrices \dz{more explanation on the small matrices?} needed by
the computation. 

To grow a DAG, FlashMatrix allows virtual matrices of different shapes
(Figure \ref{fig:kmeans} (b)). To simplify evaluation and data flow, 
all virtual matrices in the internal matrix nodes need to have the same
\textit{partition dimension}. \textit{Sink matrices} must be an edge node
in the DAG and will be materialized.
%, because
%any computation that uses these \textit{virtual matrices} cannot be connected
%to the same DAG.  

Access to elements of a \textit{sink matrix} triggers materialization of a DAG.
FlashMatrix by default saves only the computation results of sink matrices.
In exceptional cases, especially for iterative algorithms,
FlashMatrix will materialize some non-\textit{sink matrices} to avoid
redundant computation and I/O across iterations.  We allow users to
set a flag on non-\textit{sink matrices} to cache the materialized data in memory
or SSDs during computation, similar to caching an RDD in Spark.

\begin{figure}
	\centering
	\includegraphics[scale=0.6]{FlashMatrix_figs/materialize.pdf}
	\caption{Materialization of partitions of matrices in a DAG.}
	\label{fig:mater}
\end{figure}

FlashMatrix partitions matrices in a DAG in the partition dimension and
materializes partitions separately (Figure \ref{fig:mater}). This is possible
because all matrices, except sink matrices, share the same partition dimension. 
A partition $i$ of a virtual matrix requires data only from partitions
$i$ of the parent matrices.
%Owing to
%the storage scheme of in-memory matrices, partitions $i$ of all in-memory
%matrices are stored on the same NUMA node, which minimizes remote memory access.
When materializing a sink matrix, each thread computes partial
aggregation results on the partitions assigned to the thread. 
Then, FlashMatrix merges per-thread partial results to construct the output.

FlashMatrix uses two-level partitioning of dense matrices
to reduce data movement between SSDs and CPU. It assigns I/O-partitions
to a thread as a parallel task.
%A small partition size (\rb{How?}) balances the overhead of accessing a partition,
%computation, skew, and memory consumption. 
It further splits I/O-partitions into Pcache-partitions (processor cache
partitions) at run time and each thread materializes one Pcache-partition
at a time. 
%Materialization of a CPU-level partition
%is triggered recursively. As shown in 
Figure \ref{fig:mater} shows how Pcache-partitions
materialize recursively. Matrix \textit{CNT} triggers materialization of
Pcache-partitions of \textit{I} and \textit{one}, which in turn triggers 
partitions of \textit{D}, and so on. Eventually, the process triggers data access
to an I/O-partition of input matrix \textit{X} from SSDs. Upon materializing
output to a sink matrix, the thread passes the output as input to the subsequent
GenOp, instead of materializing the next Pcache-partition in the same thread.
A Pcache-partition is sufficiently small to fit in the CPU cache so that
the partition still resides in the CPU cache when the subsequent GenOp consumes
it. This significantly reduces data movement between CPU and memory. In each
thread, all intermediate matrices have only one Pcache-partitions materialized
at any time to reduce CPU cache pollution and increase CPU cache hits.
%To reduce CPU cache polution, a CPU-level partition is discarded once it is
%used by all children matrices.

%In a DAG, a matrix may be
%required by multiple GenOps. As such, each matrix always buffers one materialized
%CPU-level partition in each thread to avoid redundant computation.

% TODO
%To keep data in CPU cache as long as possible, we reuse the memory buffers
%to reduce the number of memory buffers used in the computation and avoid CPU
%cache polution.

\subsection{Parallel execution and I/O access}
FlashMatrix dispatches computation to threads so that they
%When materializing a DAG in parallel, FlashMatrix dispatches computation tasks
issue large reads and writes to SSDs, while still achieving good load balancing.
%
FlashMatrix uses a global task scheduler to assign 
I/O-partitions to threads dynamically. Initially,
the scheduler assigns multiple contiguous I/O-partitions to a thread.
The thread reads these sequentially in a single I/O.
%so that the thread can read these I/O-level partitions from a matrix with
%a single I/O. 
As the computation nears an end, the scheduler dispatches single I/O-partitions. 
The scheduler always ensures that all threads are working on I/O-partitions that are adjacent to
each other on storage.
In this way, if the DAG stores the materialized result of a non-\textit{sink matrix}, 
contiguous regions make it easier for the file system to merge
writes from multiple threads, which helps to sustain write throughout and reduces
write amplification \cite{}.

%For a block matrix with many TAS matrices, we parallelize the computation and
%I/O access differently. One of the goals is to reduce memory consumption.
%Instead of getting the row/column range from all TAS matrices before performing
%computation, we read the row/column range from some TAS matrices first, perform
%computation and move on to the next TAS matrices in the same range. This is
%very helpful if the computation is aggregation.
