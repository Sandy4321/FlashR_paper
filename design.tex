\section{Design}

FlashMatrix is a matrix-oriented programming framework for general-purpose
data analysis. This work mainly focuses on dense matrices and scales
dense matrix operations beyond memory capacity by utilizing fast I/O devices
such as solid-state drives (SSDs) in a non-uniform memory architecture (NUMA).
FlashMatrix uses R as its main programming interface and executes R code
automatically in parallel and out of core.

Figure \ref{fig:arch} shows the architecture of FlashMatrix. The main programming
interface is a small number of generalized matrix operators (GenOps). As such,
FlashMatrix only focuses on optimizations on these GenOps
to greatly simplify the implementation and improve the expressiveness of
the framework. The optimizer in FlashMatrix aggressively merges operators to
reduce CPU cache misses and I/O accesses and achieve better parallelization.
FlashMatrix stores large matrices on SSDs through SAFS \cite{safs},
a user-space filesystem for a large SSD array, to fully utilize high I/O
throughput of the SSDs and deploys a set of I/O optimizations to improve
its sequential I/O throughput \cite{SEM_SpMM}.

\begin{figure}
\centering
\includegraphics[scale=0.3]{./architecture.pdf}
\caption{The architecture of FlashMatrix.}
\label{fig:arch}
\end{figure}

FlashMatrix provides an matrix-oriented functional programming interface.
It exposes the GenOps in its R interface and reimplements many functions
in the R \textit{base} package with the GenOps. Every function operates on
one or more matrices and outputs a new matrix. As such, the matrices in
FlashMatrix are immutable. The functions provided by the R interface are
categorized into seven classes:
\begin{itemize}
	\item The generalized matrix operators (GenOps).
	\item The construction functions that create vectors and matrices of
		the specified size and fill them with specified data (e.g., sequence
		number or random number).
	\item The conversion functions that convert between FlashMatrix matrices and
		R matrices to enable interaction between FlashMatrix and the R framework.
	\item The transformation functions that change the shape of a matrix.
		The commonly used functions include matrix transpose, matrix combination
		such as \textit{rbind} and \textit{cbind} and submatrix extraction.
	\item Element-wise operations such as matrix addition and subtraction.
	\item Aggregation operations such as summation.
	\item BLAS matrix multiplication.
\end{itemize}

%\subsection{SAFS}

%SAFS \cite{safs} is a user-space filesystem for a high-speed SSD array in
%a NUMA machine. It is implemented as
%a library and runs in the address space of its application. It is deployed
%on top of the Linux native filesystem. SAFS was originally designed for
%optimizing small I/O accesses. However, FlashMatrix accesses data in matrices
%sequentially and
%generates much fewer but much larger I/O. Therefore, we provide additional
%optimizations to maximize sequential I/O throughput from a large SSD array.

%The first optimization is to enable polling for I/O to reduce thread context
%switches. On a high-speed SSD array, the latency caused by a thread context
%switch becomes noticeable under a sequential I/O workload and it becomes
%critical to avoid thread
%context switch to gain I/O performance. If the computation in application
%threads does not saturate CPU, SAFS will put the application threads into
%sleep while they are waiting for I/O. This results in many thread context
%switches and underutilization of both CPU and SSDs. To saturate I/O,
%an application thread issues asynchronous I/O to SAFS and poll for I/O
%completion after completing all computation available to it. Polling avoids
%a thread from being switched out during I/O access and effectively maximizes
%I/O throughput of a high-speed SSD array.

%To better support access to many files simultaneously, SAFS stripes data in
%a file across SSDs with a different striping order for each file. Due to
%the sequential I/O workload, FlashMatrix stripes data across SSDs with a large
%block size, on the order of megabytes, to increase I/O throughput and reduce
%write amplification on SSDs \cite{Tang15}. Such a large block size may cause
%storage skew for small files on a large SSD array if every file stripes data
%in the same order. Using the same striping order also causes skew in I/O access.
%Therefore, SAFS generates a random striping order for each file to evenly
%distribute I/O among SSDs. SAFS stores the striping order with a file for
%future data retrieval.

%When accessing a file sequentially from SSDs, we maintain a set of memory buffers
%for I/O access to reduce the overhead of memory allocation.
%We use large I/O to increase I/O throughput. As such, we need to allocate
%a large memory buffer for I/O access.
%The operating system usually allocates a large memory buffer with \textit{mmap()}
%and populates the buffer with physical pages when it is used. It is
%computationally expensive to populate
%large memory buffers frequently. When accessing high-throughput I/O devices,
%such overhead can cause substantial performance loss. Therefore, we keep a set
%of memory buffers allocated previously and reuse them for new I/O requests.

\subsection{Dense matrices}
Dense matrices are the main data types in FlashMatrix. A vector is stored
as a one-column dense matrix. In FlashMatrix, a dense matrix can be physically
stored in memory or on SSDs or represented by a sequence of computation.

%A matrix has two identifiers: the \textit{matrix identifier} that indicates
%the matrix itself and the \textit{matrix data identifier} that indicates
%the data in the matrix. When a matrix is cloned or transposed, the new matrix
%has a different \textit{matrix identifier} but shares the same
%\textit{matrix data identifier} with the original matrix.

\subsubsection{Tall-and-skinny matrix} \label{sec:tas_mat}

FlashMatrix optimizes for tall-and-skinny (TAS) dense matrices due to their
common existence in many data analysis tasks. As suggested by the name,
a dimension of these matrices is much larger than the other (Figure \ref{fig:tas_mat}).
In data analysis and machine learning, many data matrices contain a large
number of samples with a relatively small number of attributes, so the shape
of the data matrices is usually tall and skinny; if a data matrix has many
attributes, the first step is usually dimension reduction due to the curse of
dimensionality \cite{} and the operation results in a tall-and-skinny matrix.
FlashMatrix specifically optimizes for TAS dense matrices with tens of columns.
This section describes optimizations on TAS matrices and the same optimizations
can be applied to short-and-wide matrices.

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{./dense_matrix.pdf}
	\caption{The format of a tall-and-skinny dense matrix.}
	\label{fig:tas_mat}
\end{figure}

FlashMatrix uses two-level horizontal partitioning on TAS matrices for efficient
data access to SSDs and main memory (Figure \ref{fig:tas_mat}). Both in-memory
and external-memory TAS matrices are first partitioned horizontally into
I/O-level partitions. All elements in an I/O-level partition are stored
contiguously regardless of the data layout in the matrix. For an in-memory matrix,
the partition size determines the size of contiguous memory required in memory
allocation (see Section \ref{sec:mem}). For an external-memory matrix, each I/O
access reads the entire I/O-level partition and thus its size determines an I/O
size (usually in the order of megabytes). The number of rows in an I/O-level
partition is always $2^i$. As such, for a column-major TAS matrix, data in
columns of a partition is well aligned in memory to help CPU vectorization.
We further split an I/O-level partition
horizontally into CPU-level partitions for computation. We use a small
CPU-level partition (in the order of kilobytes) so that it fits in CPU L1/L2
cache to reduce CPU cache misses when evaluating a sequence of matrix
operations (Section \ref{sec:lazy_eval}). FlashMatrix determines the number
of rows in a CPU-level partition based on the number of columns in a matrix.

%\begin{itemize}
%\item NUMA-level partitioning: When a TAS
%matrix is stored in memory, it is partitioned across NUMA nodes to achieve
%data locality and fully utilize memory bandwidth. NUMA-level partitioning
%maps partitions of different vectors/matrices involved in computation
%to the same NUMA node to reduce inter-processor communication. As such,
%the partition size (the number of rows in a partition) is a global parameter
%and is not affected by the number of columns in a matrix.
%\end{itemize}

FlashMatrix supports both row-major and column-major matrix layout. As such,
we avoid physical data copy for matrix operations such as matrix transpose.
Furthermore, each GenOp has its own preferred matrix layout. Therefore,
FlashMatrix optimizes matrix operations for both data layouts. The layout
of an output matrix is generally determined by a GenOp. Due to
the partitioning scheme shown above, a column in a column-major TAS matrix
is not stored contiguously (Figure \ref{fig:tas_mat}). 

\subsubsection{Virtual matrix} \label{virt_mat}
In many cases, we do not need to store the data of a matrix physically. Instead,
we can compute and generate its data on the fly. We refer to such matrices as
\textit{virtual matrices}, which store computation and potentially the reference
to some other matrices required by the computation. A simple example is a matrix
with all elements having the same value. For such a matrix, we only need to store
a single value and construct its matrix partitions during computation.

\textit{Virtual matrices} are essential for lazy evaluation (Section
\ref{sec:lazy_eval}). All GenOps may output \textit{virtual matrices} that
represent the computation result and only store the computation of the GenOps
and the references to the input matrices of the GenOps. This strategy is
essential for both in-memory and external-memory optimizations to improve
performance. It significantly reduces data access to memory and SSDs and
memory allocation overhead.

%\subsubsection{Cached matrix}
%Memory cache is necessary for external-memory matrices on a machine with
%substantial memory.
%Even though SSDs have significantly improved I/O performance, they are still
%an order of magnitude slower than DRAM. Unfortunately, we cannot rely on
%the page cache in SAFS \cite{sa-cache} to buffer some portion of a dense matrix
%because streaming a matrix to memory always evicts existing data in the page
%cache and generates zero cache hits. Therefore, we explicitly cache some portion
%of a dense matrix.

%We store a matrix cache as an in-memory dense matrix.
%To effectively cache data in a dense matrix, we store a tall matrix in
%column-major and cache the first few columns; we store a wide matrix in row-major
%and cache the first few rows. As such, when computation requests an I/O-level
%partition of a matrix, we only need to issue a single I/O request to read
%the remaining columns or rows to reconstruct the I/O-level partition.

%We use a write-through policy for the matrix cache. As such, even when part of
%a dense matrix is cached, we keep a complete copy of the dense matrix on SSDs.
%The benefit of a write-through policy is to overlap computation and I/O when
%a dense matrix is created and avoid I/O latency when removing the cache.

\subsubsection{A group of dense matrices} \label{sec:mat_group}
FlashMatrix represents a tall matrix with many columns with a group of
tall-and-skinny matrices and a wide matrix with a group of short-and-wide
matrices. We construct a special \textit{virtual matrix} to represent
the group of dense matrices and decompose a matrix operation on the group of
matrices into multiple matrix operations on individual matrices in the group to
take advantage of the optimizations on matrix operations on TAS matrices.
This strategy resembles 2D-partitioning on a dense matrix.

%\begin{figure}
%	\centering
%	\includegraphics[scale=0.5]{./matrix_group.pdf}
%	\caption{A group of matrices to form a tall matrix with many columns (a)
%	and a wide matrix with many rows (b).}
%	\label{fig:mat_group}
%\end{figure}

%A group of matrices should be stored in a single SAFS file to reduce the number
%of SAFS files.

%append an element to a vector can be implemented as physically appending the element
%to the vector. The result vector becomes the new vector, and the original vector
%becomes the sub-vector of the new vector.

\subsection{Generalized computation operations} \label{sec:genop}
To improve generality and simplify the implementation, FlashMatrix provides
only four generalized operators (GenOps) on matrices: \textit{inner product},
\textit{apply}, \textit{aggregation} and \textit{groupby}. Each of the operators
represents a data access pattern and accepts user-defined functions (UDF) to
perform actual computation. GenOps accept the pointers to UDFs at runtime
to support dynamic programming languages such as R.

\textit{Inner product} is a generalized matrix multiplication. It replaces
multiplication and addition in matrix multiplication with two UDFs,
respectively. The first UDF performs binary operation and the second one
performs aggregation. We define many operations
with inner product. For example, we use inner product to compute various
pair-wise distance matrices of vectors such as Euclidean distance and
Hamming distance. For dense matrices, we mainly focus on
optimizing two cases: inner product of a wide matrix and a tall matrix (denoted
with \textit{inner\_prod\_wide}) and inner product of a tall matrix and a small
matrix (denoted with \textit{inner\_prod\_tall}). \hl{It is usually impractical to
materialize inner product of a large tall matrix and a large wide matrix due to
the large computation and space complexity. As such, we rely on users to
transform this form of inner product to the other two forms to reduce computation
and space.} Even though we can implement matrix multiplication with inner product,
FlashMatrix relies on BLAS to implement matrix multiplication for
floating-point matrices to achieve speed and precision required by
many numeric libraries, such as eigensolvers \cite{anasazi, FlashEigen}.

\textit{Apply} is a generalized form of element-wise operations and has
multiple variants. The simplest variant (denoted with \textit{sapply}) is
a generalized element-wise unary operation whose UDF takes one element at a time
in a matrix and outputs one element. We can use it to implement many unary
operations such as negation, square root or element type casting
on a matrix. The second variant (denoted with \textit{mapply2}) is a generalized
element-wise binary operation whose UDF takes one element from each input
matrix and outputs a single element. We use it to implement many binary
matrix operations such as matrix addition and subtraction. The third
(denoted with \textit{mapply\_row}) and the fourth variants (denoted with
\textit{mapply\_col}) are perform element-wise
binary operation on the input vector with every row or column of the input
matrix and output a matrix with the same shape as the input matrix.
%Although
%\textit{mapply\_row} and \textit{mapply\_col} can also be implemented by
%replicating the input vector to create a matrix and applying \textit{mapply2}
%on the input matrix and the created matrix, \textit{mapply\_row} and
%\textit{mapply\_col} avoid CPU cache polution to improve performance.

\textit{Aggregation} takes multiple elements and an aggregation UDF and outputs
a single element. It has three variants on a matrix.
The first variant (denoted with \textit{agg}) aggregates over all elements on a matrix.
Matrix summation is an example of the first variant. The second (denoted with
\textit{agg\_row}) and the third variants (denoted with \textit{agg\_col})
compute aggregation over each individual row or column. \textit{rowSums}
and \textit{colSums} in R are examples. To enable parallelization,
the aggregation UDF needs to provide an \textit{aggregate} function that runs
on the elements of the input matrix and a \textit{combine} function that runs
on the partially aggregated results.

\textit{Groupby} on a matrix groups rows (\textit{groupby\_row}) or columns
(\textit{groupby\_col}) of a matrix based on a vector of categorical values
and invokes an aggregation UDF on the rows or
columns associated with the same categorical value. Matrix \textit{groupby}
can be used in many classification and clustering algorithms that compute
aggregation on the data points in a class or in a cluster. Aggregation UDFs
in \textit{groupby} are the same to the ones in \textit{aggregation} and
each requires an \textit{aggregate} function and a \textit{combine} function
to enable parallelization.

\subsection{Vectorized user-defined function}
All of the GenOps take vectorized UDFs (VUDFs), which takes a vector of
elements instead of an individual element. Most of the UDFs of the GenOps
perform computation on individual elements, which potentially results in
significant function call
overhead. By transforming the operations on individual elements to the ones
on a vector, we can significantly reduce the number of function calls and
amortize their overhead.

We have three types of VUDFs to support the GenOps in Section \ref{sec:genop}.
A VUDF type may have multiple forms to allow GenOps to transform
operations based on the data layout of the matrix to increase the length of
an input vector in a VUDF and increase amortization of function call overhead.
\begin{itemize}
	\item A \textit{unary} VUDF (denoted with \textit{uVUDF}) has only one form,
		which takes a vector as input and outputs a vector of the same length.
	\item A \textit{binary} VUDF has three forms: the first form (denoted with
		\textit{bVUDF1}) takes two vectors of the same length and outputs
		a vector of the same length as the input vectors; the second form (denoted
		with \textit{bVUDF2}) takes a vector as the left argument and a scalar
		as the right argument and outputs a vector with the same length as
		the input vector; the third form (denoted with bVUDF3) takes a scalar
		as the left argument and a vector as the right
		argument and outputs a vector. The reason of having both the second and third
		forms is to support non-commutative binary operations such as division and
		subtraction.
	\item An \textit{aggregation} VUDF is composed of two functions:
		\textit{aggregate} and \textit{combine}. Both functions may have two
		forms: the first one (denoted with \textit{aVUDF1}) takes a vector and
		outputs a scalar; the second one (denoted with \textit{aVUDF2}) takes
		two vectors of the same length and outputs a vector. For many
		aggregation VUDFs such as summation, \textit{aggregate} and
		\textit{combine} are the same and have both \textit{aVUDF1} and
		\textit{aVUDF2} forms. For some aggregation such as \textit{count},
		\textit{aggregate} and \textit{combine} are different.
\end{itemize}

FlashMatrix implements many commonly used VUDFs by default. These VUDFs wrap
basic operations built in many programming languages and libraries. For example,
FlashMatrix provides arithmetic operations such as \textit{addition} and
\textit{subtraction}, relational operations such as \textit{equal to} and
\textit{less than}, logical operations such as \textit{logical AND} and
\textit{logical OR}, as well as commonly used math functions such as computing
absolute values and square root. FlashMatrix also provides a set of VUDFs to
cast primitive element types.

For each basic operation wrapped in a VUDF, FlashMatrix provides multiple
VUDF implementations to support different element types. To reduce the number
of binary VUDF implementations,
FlashMatrix only provides the implementations that take two input arguments of
the same type. If a GenOp gets two matrices with different
element types, it first casts the the element type of one matrix to match
the other. Type casting follows the usual arithmetic conversions \cite{}
commonly seen in many programming languages. The type casting operation is
implemented with \textit{sapply} and is performed lazily (in Section
\ref{sec:lazy_eval}).

We balance the amortization of the function call overhead and CPU cache
misses. Because VUDFs operate on vectors, we can no longer keep the input data
of a VUDF in CPU registers. To reduce latency of accessing data in VUDFs,
the input data has to be small enough to be kept in the CPU L1 cache. On the
other hand, passing a longer vector to a VUDF amortizes the overhead of
function calls more aggressively. We choose 128 as the maximum length of
the input vector of a VUDF.
Futher increasing the length does not have noticeable performance improvement.
\dz{I need to show this in the experiment.}

We use CPU vector instructions such as AVX \cite{avx} to accelerate
the computation in a VUDF. The current implementation of FlashMatrix heavily
relies on auto-vectorization
of a compiler such as GCC to vectorize computaion. It provides hints and
transforms code to help auto-vectorization. For example, a VUDF in FlashMatrix
frequently operates on vectors with data well aligned in memory and of
the length defined at compile time, so we inform the compiler of the data alignment
and the vector length. Some compilers do not automatically vectorize
aggregation operations well. We manually create a small vector of reduction
variables, flatten the loop and transform the original aggregation operation
into aggregation onto the vector of reduction variables.

\subsection{Lazy evaluation} \label{sec:lazy_eval}
FlashMatrix evaluates matrix operations lazily. Evaluating individual
matrix operations results in significant data movement between CPU and SSDs
and frequent memory allocation in in-memory execution.
Lazy evaluation allows to fuse matrix operations to minimize data movement,
reduce memory allocation overhead and improve parallelization \cite{Ching12}.

\begin{figure}
	\centering
	\includegraphics[scale=0.7]{./sd.pdf}
	\caption{A directed acyclic graph of computing standard deviation on
	a matrix with missing values.}
	\label{fig:DAG}
\end{figure}

FlashMatrix constructs a directed acyclic graph (DAG) \cite{} to represent
computation formed by a sequence of matrix operations evaluated lazily
(Figure \ref{fig:DAG} (a)). A lazily evaluated GenOp outputs a \textit{virtual matrix}
to capture the matrix computation and the input matrices. As such, a computation
DAG is formed with a set of \textit{virtual matrix} nodes (shown as rectangles)
and computation nodes (shown as cycles). We refer to the input matrices of
a computation node as the parent matrices and the output matrix as the child
matrix.
The computation nodes may contain some immutable computation state such as
scalar variables and small matrices involved in the matrix computation.
A computation node may take more than two matrices as input and always outputs
one matrix. A matrix node can be used by multiple computation nodes as
an input matrix. Although the \textit{virtual matrices} do not need to have
the same shape, FlashMatrix requires all \textit{virtual matrices} in the DAG
has the same \textit{long dimension} size (Figure \ref{fig:DAG} (b)) to
simplify evaluation and data flow of a DAG (a \textit{long dimension}
refers to the matrix dimension with the larger size).

FlashMatrix allows lazy evaluation on all GenOps.
FlashMatrix always lazily evaluate the GenOps that output matrices with
the same \textit{long dimension} size because they can be easily connected
with other nodes in a DAG. The examples of such GenOps are \textit{sapply}
and \textit{mapply2}, which always output a matrix with the same size as
the input matrices, and \textit{agg\_row} on a tall matrix and \textit{agg\_col}
on a wide matrix, which output a matrix with a different shape but with
the same \textit{long dimension} size as the input matrix. FlashMatrix also
supports lazy evaluation on the GenOps that output matrices with different
\textit{long dimensions} sizes. The output matrices of these GenOps are
\textit{sink matrices} in a DAG because any computation
that uses a \textit{sink matrix} as input cannot be connected to the same DAG.

To enable lazy evaluation, all matrices in FlashMatrix are immutable.
As such, \textit{virtual matrices} can generate the same result every time
they are materialized. This requires every matrix operation to generate
a new matrix. FlashMatrix garbage collects a matrix when there are not any
references to it.

\subsection{Matrix materialization} \label{sec:materialize}
Lazy evaluation postpones the computation in matrix operations, but we
eventually have to the actual computation.

FlashMatrix allows to materialize any \textit{virtual matrix} in a DAG.
By default, FlashMatrix only materializes the \textit{sink matrices} in a DAG.
However, many iterative algorithms need to physically materialize the TAS
\textit{virtual matrices} to avoid redundant computation and I/O. As such,
FlashMatrix allows users to set a flag on a TAS \textit{virtual matrix} to
inform FlashMatrix to save materialized results during computation.
\dz{FlashMatrix can materialize a DAG from multiple virtual matrices.}

We partition matrices on a DAG in the \textit{long dimension} for materialization
and parallelization (Figure \ref{fig:DAG} (b)). All \textit{virtual matrices}
except \textit{sink matrices} on a DAG have the same \textit{long dimension}
size and they all share the same partition size in the \textit{long dimension}.
As such, a partition $i$ of a \textit{virtual matrix} only requires data from
partitions $i$ of the parent matrices and thus partitions in a matrix can be
evaluated independantly. By default, FlashMatrix discards data in a partition
immediately once the data is used by all children matrices.
%Owing to the storage scheme for
%in-memory matrices, the data of all in-memory matrices in a partition are stored
%on the same NUMA node to minimize remote memory access.

A \textit{sink matrix} uses the same partitioning as other matrices in a DAG
and performs a form of aggregation operation on the partitions
of its parent matrix. Each thread computes partial aggregation result from
the partitions of its parent matrix assigned to the thread and stores partial
aggregation in a local memory buffer. Afterwards, FlashMatrix combines
the partial aggregation results to compute the final result.

We consider \textit{sink matrices} small and by default materialize them
in memory. For example, \textit{agg\_row} on a wide matrix and
\textit{agg\_col} on a tall matrix outputs a vector. The maximal length of
the vector is smaller than $\sqrt{N}$, where $N$ is the number of elements
in the input matrix. As such, we can always keep it in memory. For most of machine
learning and data analysis tasks, the output matrix of the inner product of
a wide matrix with a tall matrix is considered small and is by default kept
in memory because the long dimension of these matrices is usually much larger
than the short dimension in these tasks.

%How is a DAG traversed during materialization?
%When materializing a partition on a \textit{virtual matrix}, FlashMatrix requires
%all input partitions have been materialized or read from SSDs.

FlashMatrix uses two levels of partitioning when materializing computation.
It assigns I/O-level partitions (Section \ref{sec:tas_mat}) to a thread as
computation tasks for parallelization. We choose a relatively small partition
size to balance the overhead of accessing a partition, parallelization skew
and memory consumption. A thread further splits an I/O-level partition into
CPU-level partitions and materializes one CPU-level partition at a time.
When a CPU-level partition is materialized, it is passed to the subsequent
operation in the DAG. A CPU-level partition is sufficiently small to fit in
the CPU cache so that it still resides in CPU cache when it is consumed by
the subsequent computation. In a DAG, a matrix may be required by multiple
GenOps so each matrix always buffers one materialized CPU-level partition in
each thread. \dz{We need to identify transpose of a matrix.}

%FlashMatrix uses a global task scheduler to assign tasks to threads dynamically
%for load balancing and I/O merging. However, SSDs require large
%writes to achieve sustainable write throughout and reduce write amplification
%\cite{}. As such, FlashMatrix delays writing the materialized partitions to
%SSDs and merge multiple materialized partitions into a single write because
%the global task scheduler assigns consecutive partitions to threads.

%To keep data in CPU cache as long as possible, we reuse the memory buffers
%to reduce the number of memory buffers used in the computation and avoid CPU
%cache polution.

\subsection{Memory management} \label{sec:mem}
FlashMatrix manages memory allocation for large in-memory matrices and memory
buffers for accessing data on SSDs to reduce memory allocation overhead.

Frequent large memory allocation is expensive. Linux uses \textit{mmap} to
allocate large memory in the order of megabytes and relies on page fault to
populate the memory. Frequent page faults prevent computation from fully
utilizing CPUs in a large parallel machine. The memory allocation overhead
degrades performance significantly in FlashMatrix because each of its matrix
operations generates a new matrix and causes frequent memory allocation.

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{./matrix_mem.pdf}
	\caption{Memory layout of a tall-and-skinny dense matrix.}
	\label{fig:mat_mem}
\end{figure}

FlashMatrix divides an in-memory matrix into fixed-size memory chunks and
recycles memory chunks to reduce memory allocation overhead (Figure
\ref{fig:mat_mem}). We perform matrix operations on I/O-level partitions
and only require an I/O-level partition to be stored in contiguous memory.
Therefore, we can store a matrix in fixed-size memory chunks, as long as
a memory chunk is sufficient to store an I/O-level partition. The size of
a memory chunk is a global parameter and is the same for all matrices.
As such, memory chunks allocated for a matrix can be used by another matrix
with a totally different shape. In practice, the memory chunk size may
not be divisible by an I/O-level partition size. Therefore, the memory chunk
size needs to be much larger than the I/O-level partition size to increase
memory utilization. We use 64MB as the default memory chunk size.

Similarly, we maintain per-thread memory buffer pools to store data read from
SSDs. These memory buffers need to be
the same size as I/O-level partitions, which is in the order of megabytes
to maximize I/O throughput of an SSD. Due to asynchronous I/O, a worker thread
needs to allocate a small number of memory buffers for each matrix
during computation. Because all partitions in a vector or a matrix have
the same size, the memory buffers are reused for processing other partitions
of vectors or matrices.

\subsection{Implementation of GenOps with VUDF}

After FlashMatrix reads partitions from SSDs, it needs to invoke VUDFs on
the elements of the partitions intelligently to reduce the overhead of function
calls. Different GenOps choose different forms of VUDFs based on the data layout
and the shape of the input matrices. Most of the GenOps requires a matrix with
a specific data layout to increase the amortization of function call overhead.

Many of the GenOps favor the column-major order for a tall-and-skinny matrix
and the row-major order for a short-and-wide matrix. These data layouts not only
increase the length of a vector passed to a VUDF but also align data in memory
for CPU vectorization. For a tall matrix, the column-major order ensures data
in each column in a partition to be well aligned in memory owing to the matrix
format and the partition size (Figure \ref{fig:tas_mat}), regardless of the number
of columns in the matrix; similarly, the row-major order aligns data in rows of
a partition. Before invoking VUDFs on
CPU-level partitions, a GenOp such as inner product may convert the data layout
of a CPU-level partition to the preferred layout if an input matrix does not
have the preferred layout.
Because a CPU-level partition can fit in the CPU cache, data layout conversion
does not increase data access to main memory. 

Some GenOps can be applied to matrices with any data layout and any shape
efficiently. For example, \textit{sapply} and \textit{mapply2} only require
the input matrices and the output matrix to have the same data layout. For tall
column-major matrices and wide row-major matrices, each partition has long
columns and long rows, respectively. As such, these GenOps can invoke a VUDF
on the long columns and rows. For tall row-major matrices and wide column-major
matrices, all rows and columns in a partition is stored in a single piece of
memory. As such, these GenOps only need to invoke a VUDF once on all elements
in a partition. A similar strategy is applicable to \textit{agg}.

Given a matrix with the preferred data layout, FlashMatrix automatically
selects the right form of a VUDF for a given GenOp to increase the length of
the vectors passed to a VUDF. For example, when applying \textit{mapply\_col}
on a tall column-major matrix, FlashMatrix chooses the \textit{bVUDF1} form to
perform computation on a column from the matrix with the vector; when applying
\textit{mapply\_row} on a tall column-major matrix, FlashMatrix choose
the \textit{bVUDF2} form to perform computation on a column from the matrix
with an element from the vector. When applying \textit{inner\_prod\_tall} on a tall
column-major matrix, FlashMatrix uses the \textit{bVUDF2} form of the first
VUDF to computes the outer product of a column from the left matrix and a row
from the right matrix and uses the \textit{bVUDF1} of the second VUDF to compute
the final result. Because \textit{inner\_prod\_tall} operates on a CPU-level
partition, all intermediate results are kept in CPU cache and thus we only need
to read the input partition from main memory once and write the final result
back to main memory. \textit{inner\_prod\_wide} invokes the
\textit{bVUDF1} form of the first VUDF on a row from the left matrix and a column
from the right matrix and invokes the \textit{aVUDF1} form of the second VUDF
on the output from the first VUDF to compute an element in the output for
the input partitions.

%Both \textit{agg\_row} and \textit{agg\_col} work better on the preferred data
%layout if the aggregation VUDF has the same \textit{aggregate} and \textit{combine}
%function with both \textit{aVUDF1} and \textit{aVUDF2} forms. For these aggregation
%VUDFs, we can transform \textit{agg\_row} on a column-major matrix to applying
%\textit{aVUDF2} to columns of a partition. Similar transformation is applied
%to \textit{agg\_col} on a row-major matrix.

%Inner product chooses different forms of VUDFs for input matrices with different
%shapes. The first UDF of inner product is a binary UDF and the second one is an
%aggregation UDF. For the sake of efficiency, inner product requires the second
%UDF to have the same \textit{aggregate} and \textit{combine} function with both
%\textit{aVUDF1} and \textit{aVUDF2} forms.

%Unlike other GenOps, \textit{groupby\_row} and \textit{groupby\_col} are the only
%GenOps that do not favor the column-major order for a tall-and-skinny matrix and
%the row-major order for a short-and-wide matrix. \textit{groupby\_row} sorts
%rows in a partition based on the categorical values associated with each row
%and apply the aggregation VUDF on rows directly.

\subsection{Implementation of GenOps on a group of matrices}

When applying a GenOp on a group of matrices,
we decompose the computation into multiple GenOps and apply them to individual
matrices in the group if the GenOp supports decomposition. Decomposing computation
to individual matrices reduces memory copy and increases CPU cache hits and thus
can significantly improve performance. For the GenOps that cannot be decomposed,
we combine the individual matrices on the fly and apply the GenOps on the combined
matrix directly.

We can apply some of the GenOps to individual matrices directly. For example,
\textit{sapply} and \textit{agg} run on individual matrices directly regardless
of the shape and data layout of individual matrices. Other GenOps may be applied
to individual matrices directly without transformation if the input matrices
have certain shape. For a \textit{tall matrix group}, we can apply
\textit{mapply\_col} and \textit{agg\_col} to individual matrices directly.
Similarly, we can apply \textit{mapply\_row} and \textit{agg\_row} to
individual matrices directly for a \textit{wide matrix group}. %\textit{mapply2}
%requires the matrices in the two input matrix groups to have the same shape.
%\textit{Groupby\_row} on a \textit{tall matrix group} can be decomposed and
%applied to individual matrices; \textit{groupby\_col} on a \textit{wide matrix group}
%can be decomposed in a similar fashion.

Applying other GenOps to a matrix group requires transformation. If an aggregation
operation provides a combine function, applying \textit{agg\_row} to a group of
tall matrices is transformed into two steps: apply the aggregation function on
each row of individual matrices and apply the combine function on aggregation
results. Similarly, the same strategy is used for \textit{agg\_col} on a group
of wide matrices. When applying \textit{mapply\_row} to a group of tall matrices,
we break the vector into parts to match the number of columns in the individual
matrices in the group. Similarly, we break the vector into parts to match the number
of rows in the individual matrices for \textit{mapply\_col}.

%FlashMatrix decomposes inner product on a matrix group in favor of minimizing
%the amount of data written to SSDs (Figure \ref{fig:inner_prod}).
%For \textit{inner\_prod\_tall}, we first partition the right matrix vertically
%so that the inner product of the left matrix and a vertical partition outputs
%part of a final output matrix. The number of vertical partitions determines
%the number of runs required to complete the inner product on the group of matrices.
%If the left matrix is stored on SSDs, the number of vertical partitions on
%the right matrix determines the amount of data read from SSDs. The vertical
%partition size determines the output matrix size in each run and affect
%the memory size. As such, we need to select the vertical partition size of
%the right matrix to balance I/O and memory consumption. We horizontally partition
%each vertical partition of the right matrix to further decompose the inner
%product. We construct a directed acyclic graph (DAG) to evalute the inner
%product lazily (Section \ref{sec:lazy_eval}). Similarly, we partition the right
%matrix and make a similar choice to balance I/O and memory consumption for
%\textit{inner\_prod\_wide}. We also construct a DAG and lazily evaluate
%the computation.

%\begin{figure}
%\centering
%\includegraphics[scale=0.4]{./inner_prod_tall.pdf}
%\vspace{-5pt}
%\caption{Decomposition inner product on a group of dense matrix.}
%\vspace{-5pt}
%\label{fig:inner_prod}
%\end{figure}
