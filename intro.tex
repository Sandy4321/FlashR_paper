% What problem are you going to solve.

The explosion of data and the increasing complexity of data analysis
generate a growing demand for parallel, scalable statistical analysis
and machine learning tools that are simple and efficient.
Simple tools need to be programmable, interactive, and extensible, 
allowing scientists to encode and deploy complex algorithms. 
Successful examples include R, SciPy, and Matlab.  Efficiency dictates that
tools should leverage modern computer architectures, including scalable
parallelism, high-speed networking, and fast I/O from memory and solid-state
storage. The current approach for utilizing the full
capacity of modern parallel systems often uses a low-level programming
language such as C and parallelizes computation with MPI or OpenMP.
This approach is time-consuming and error-prone, and requires machine learning
researchers to develop expertise in parallel programming models.
 
% How have others addressed the problem?

While conventional wisdom addresses large-scale data analysis and machine
learning with clusters
\cite{mapreduce,spark,systemml,tensorflow,petuum,graphlab}, recent works
\cite{flashgraph,gridgraph,Matveev17,hotos} demonstrate a single-machine
solution can deal with large-scale data analysis efficiently in a multicore
machine. The advance of solid-state drives (SSDs) allows us to tackle data
analysis in a single machine efficiently at a larger scale and more economically
than possible before. Previous SSD-based graph analysis frameworks
\cite{flashgraph, gridgraph, graphene}
have demonstrated the comparable efficiency to state-of-the-art in-memory graph
analysis, while scaling to arbitrarily large datasets. This work extends
these findings to matrix operations using SSDs for machine learning and
data analysis.
%\dz{we need to say more about the advantage of a single-machine solution
%compare with cluster solutions.}

% Why is it hard?

% What is the nature of your solution?
To provide a simple programming environment for efficient and scalable machine
learning, we present FlashR, an interactive R-based programming framework that
executes R code in parallel and out-of-core automatically. FlashR stores large
vectors and matrices on SSDs and overrides many R functions in the R
\textit{base} package to perform computation on these external-memory vectors
and matrices.
As such, FlashR executes existing R code with little/no modification.
FlashR focuses on optimizations in a single machine (with multiple CPUs and
many cores) and scales matrix operations beyond memory capacity by 
utilizing solid-state drives (SSDs).  
Our evaluation shows that we can solve billion row, Internet-scale 
problems on a single thick node, which can prevent the complexity,
expense, and power consumption of distributed systems when they are
not strictly necessary \cite{hotos}.

%These machines typically have multiple processors with many CPU cores and
%a large amount of memory. They are also equipped with fast flash
%memory such as solid-state drives (SSDs) to further extend memory capacity.
%This conforms to the node design for supercomputers \cite{Ang14}.

% Why is it new/different/special?

To utilize the full capacity of a large parallel machine, we overcome
many technical challenges to move data from SSDs to CPU efficiently for matrix
computations, notably the large performance disparities between CPU and memory
and between memory and SSDs. In the memory hierarchy, a memory media in a layer
is in general at least an order of magnitude faster than the memory media in
the layer below for both latency and throughput.
The ``memory gap'' \cite{Wilkes01} continues to grow, with 
the difference between CPU and DRAM performance increasing exponentially. 
There are also performance differences between
local and remote memory in a non-uniform memory architecture (NUMA), which are prevalent
in modern multiprocessor machines. 
%Even though the I/O
%performance of SSDs has advanced to outperform hard drives by a large factor,
%FDespite advances in SSD I/O performance, 
%they remain an order of magnitude slower than RAM.
%Most matrix computation engines increase data movement,
%because they perform an operation on an entire input matrix prior to moving 
%to the next operation.
% RB -- 
%On the other hand, many analysis tasks are
%data-intensive. Matrix
%formulation further increases data movement between CPU and SSDs because
%a matrix computation framework typically performs an operation
%on the entire input matrices before moving to the next operation.
% As such,
%the performance of the data analysis tasks is usually limited by memory
%bandwidth instead of computing power.

FlashR evaluates expressions lazily and fuses operations aggressively
in a single parallel execution job to minimize data movement. FlashR
builds a directed acyclic graph (DAG) to represent a sequence of matrix
operations. To increase the ratio of computation to I/O, materialization
of any matrix operation in a DAG triggers materialization of all operations
in the DAG and FlashR requires only one pass over the input matrices of
the DAG to perform all operations in the DAG.
FlashR by default keeps only the output matrices (leaf nodes) of the DAG
in memory to have a small memory footprint. When materializing a DAG,
FlashR performs two levels of matrix partitioning and reorder computation
on matrix partitions to reduce data movement in the memory hierarchy.

% What are it's key features?

We implement multiple machine learning algorithms, including principal component
analysis, logistic regression and k-means, in FlashR. We demonstrate that
with today's fast commodity storage technology, the out-of-core execution of
FlashR achieves performance comparable to their in-memory execution, even
on a large parallel machine and Amazon cloud. Furthermore, FlashR outperforms
the same algorithms in H$_2$O \cite{h2o} and Spark MLlib \cite{spark} by a factor
of $3-20$ in a large parallel machine with 48 CPU cores. In the Amazon cloud,
FlashR using only one fourth of the resources still matches or even outperforms
H$_2$O and Spark MLlib. This suggests that FlashR is a much
more cost-effective solution for large-scale data analysis in the cloud.
FlashR effortlessly scales to datasets with billions
of data points and its out-of-core execution uses a negligible amount of memory
compared with the dataset size. In addition, FlashR executes the R functions
in the R MASS \cite{mass} package with little modification and outperforms
the execution of the same functions in Revolution R Open \cite{rro} by more
than an order of magnitude.

Given its high-level array-oriented programming interface and superior performance,
we argue that FlashR significantly lowers the requirements for writing
parallel and scalable implementations of machine learning algorithms. It also
offers new design possibilities for data analysis clusters, replacing memory
with larger and cheaper SSDs and processing bigger problems on fewer nodes.
FlashR is released as an open-source project at \href{http://flashx.io}{http://flashx.io}.

%\begin{itemize}
%\item identify a set of generalized matrix operations that cover all matrix
%operations in NumPy.
%\item design a set of optimizations for these generalized matrix operations
%to maximize CPU cache hits and reduce I/O access. The optimizations are tailored
%specifically for machine learning.
%\item using SSDs, EM performance matches IM performance and scale to
%Internet-scale datasets.
%\end{itemize}
