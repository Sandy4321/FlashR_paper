% What problem are you going to solve.

The National Strategic Computing Initiative (NSCI, \cite{NSCI}) puts forth a critical 
problem as we move to exascale: {\em ``Increasing coherence between the technology base used for 
modeling and simulation and that used for data analytic computing.''}  
A key challenge lies in providing statistical analysis and machine learning 
tools that are simple and efficient.
Simple tools need to be programmable, interactive, and extensible, 
allowing scientists to encode and deploy complex algorithms. 
Successful examples include R, SciPy, and Matlab.  Efficient dictates that tools should 
leverage modern HPC architectures, including scalable parallelism, high-speed networking,
and fast I/O from memory and solid-state storage.


%In today's big data era, we face the challenges in both the explosion of
%data volume and the increasing complexity of data analysis. Experiments,
%simulations and observations generate terabytes or
%even petabytes in many scientific and business areas. After collecting
%a massive amount of data, we often need to perform complex data analysis
%and machine learning techniques to extract value from the data. To handle
%the increasing volume size and effectively extract value from the data,
%the field of data mining and machine learning evolves rapidly and the community
%is developing many new algorithms to process large datasets.

% How have others addressed the problem?

Large-scale data analysis requires a large parallel machine or a cluster to
gain computation power and memory capacity. Currently,
there are two approaches of implementing parallel algorithms to process large
datasets. One can write an efficient implementation with low-level parallel
primitives such as the ones provided by MPI \cite{mpi} or OpenMP \cite{openmp}.
This approach requires expertise in parallel programming and significant
effort from programmers. The other approach is to use high-level programming
frameworks that provide high-level operations to reduce the burden of
programmers. In general, the second approach is less computationally
efficient but can significantly increase productivity and lower the barrier
of writing parallel implementations. The second approach is 
preferred in the rapidly evolving fields of machine learning and data mining.
%The high-level programming interface enables non-expert programmers
%to effectively use computation resources.

% Why is it hard?

It is challenging to provide a programming framework that has a high-level
programming interface and still achieves both generality and efficiency.
Some highly-optimized linear algebra libraries \cite{mkl, openblas, elemental,
trilinos, petsc} provides a matrix programming interface that has a limited
set of matrix operations with efficient implementations, e.g.~BLAS provide
matrix-multiplication only and not XXXX or YYY.
Users have to parallelize the remaining matrix
operations themselves that are not supported by the libraries. 
High-level programming frameworks, such as R and Matlab, provide a
general programming interface for users to express varieties of algorithms, but
do not produce efficient parallel code.

% What is the nature of your solution?
%Matrix operations are an intuitive formulation for many computation tasks,
%especially for many data analysis tasks. For example, we can store samples
%collected from an experiment in a matrix with rows corresponding to samples
%and columns corresponding to attributes.
%Consequently, we express the computation on the data matrix with a sequence
%of matrix operations. Such a formulation simplifies the implementation of
%data analysis algorithms and is very intuitive for many machine learning
%and data analysis experts.

We present FlashMatrix, a programming framework that provides a high-level
matrix-oriented functional programming interface and supports automatic
parallelization and out-of-core execution for large-scale data analysis.
Unlike most of the linear algebra libraries, FlashMatrix provides a small
set of highly-optimized generalized matrix operations (GenOps) that accept
user-defined functions (UDF) to achieve generality and reimplements many matrix
operations in the R \textit{base} package to execute R code in parallel and
out of core automatically. FlashMatrix focuses on optimizations in
a single machine and scales matrix operations beyond memory capacity by utilizing
solid-state drives (SSDs). This design choice conforms with a current trend of
hardware design that scales up a single machine for high performance computing
\cite{Ang14}, including analysis of data stored on the 
solid-state storage devices (SSDs) of I/O burst buffers \cite{bb}.
% bb = https://scholar.google.com/scholar?cluster=13235278116505344242&hl=en&as_sdt=0,21&sciodt=0,21

%These machines typically have multiple processors with many CPU cores and
%a large amount of memory. They are also equipped with fast flash
%memory such as solid-state drives (SSDs) to further extend memory capacity.
%This conforms to the node design for supercomputers \cite{Ang14}.

% Why is it new/different/special?

We overcome many technical challenges to move data from SSDs to CPU efficiently
owing to large speed disparity between CPU and memory, as well as between memory and
SSDs. The speed disparity of CPU and DRAM has increased exponentially over
the past decades \cite{Wilkes01}. While SSDs have high IOPS and sequential
I/O throughput, they are still an order of magnitude slower than DRAM.
To the contrary, many data analysis tasks are data-intensive and the matrix
formulation further increases data movement between CPU and SSDs.% As such,
%the performance of the data analysis tasks is usually limited by memory
%bandwidth instead of computing power.

Another challenge in FlashMatrix is to reduce CPU instructions. To support
the matrix operations in the R \textit{base} package, the GenOps in FlashMatrix
have to accept pointers to the UDFs at run time. The UDFs are defined to
operate on individual elements in a matrix. As such, each operation on an element
in a matrix potentially results in a function call, causing significant
computation overhead.

To move data efficiently, FlashMatrix performs lazy evaluation and aggressive
operation fusion to merge many operations in a single parallel execution.
FlashMatrix builds a directed acyclic graph (DAG) to represent all operations
in the single execution. When evaluating the computation in the DAG, FlashMatrix
optimizes data access in memory hierarchy and performs two levels of data
partitioning to achieve efficient data access and maximize data utilization in
both main memory and CPU cache and reduce data movement between memory and SSDs
as well as between CPU and memory. To access data stored on SSDs, FlashMatrix
streams data from SSDs to yeild maximal I/O throughput from SSDs.

\dz{maybe we should say something about optimization on memory allocation?}

To reduce CPU instructions, we deploy vectorized user-defined
functions (VUDFs), which operates on a vector of elements instead of
an individual element. We define multiple forms for each VUDF and automatically
select the right form for each GenOp to amortize the function call overhead.
When invoking UDFs, GenOps choose the right vector length to balance
the amortization of the function call overhead and CPU cache misses. We use
vector CPU instructions such as AVX \cite{avx} to further reduce CPU
instructions and improve CPU vectorization with better memory alignment
and code transformation.

% What are it's key features?

We implement multiple machine learning algorithms such as KMeans \cite{kmeans}
and Gaussian Mixture Model \cite{gmm} in FlashMatrix with its R programming
interface to benchmark its performance. On a large parallel machine with 48
CPU cores and fast SSDs, the out-of-core execution of these R implementations
in FlashMatrix achieves performance comparable to the in-memory execution
while significantly outperforming the ones in Spark MLlib \cite{spark}.
We believe FlashMatrix significantly lowers the requirements for writing parallel
and scalable implementations of data analysis algorithms; it also offers new
design possibilities for data analysis clusters, replacing memory with larger
and cheaper SSDs and processing bigger problems on fewer nodes.
