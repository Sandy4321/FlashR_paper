% What problem are you going to solve.

The explosion of data volume and the increasing complexity of data analysis
generate a growing demand for scalable statistical analysis and machine
learning tools that are simple and efficient.
Simple tools need to be programmable, interactive, and extensible, 
allowing scientists to encode and deploy complex algorithms. 
Successful examples include R, SciPy, and Matlab.  Efficiency dictates that
tools should leverage modern computer architectures, including scalable
parallelism, high-speed networking, and fast I/O from memory and solid-state
storage. Unfortunately, the current approach for taking advantage of the full
capacity of modern parallel systems often uses a low-level programming
language such as C or FORTRAN and parallelizes computation with MPI or OpenMP.
This approach is time-consuming and error-prone, and requires machine learning
researchers to have expert knowledge in the parallel programming models.
 
% How have others addressed the problem?

While conventional wisdom addresses large-scale data analysis with clusters
\cite{mapreduce,spark,h2o,systemml,tensorflow,petuum}, recent works
\cite{flashgraph,gridgraph,Matveev17,hotos} demonstrate a single-machine solution
can deal with large-scale data analysis problems efficiently in a multicore
machine. The advance of solid-state drives (SSDs) creates new opportunities
for a single-machine solution to tackle data analysis efficiently at a larger scale
with a cheaper price. SSD-based graph analysis frameworks \cite{flashgraph, gridgraph}
have demonstrated the comparable efficiency to state-of-the-art in-memory graph
analysis, while scaling to arbitrarily large datasets. This work extends
these findings to matrix operations using SSDs. Matrices are the most prevalent
and intuitive representation for machine learning and data analysis tasks.
\dz{we need to say more about the advantage of a single-machine solution
compare with cluster solutions.}

% Why is it hard?

% What is the nature of your solution?
To provide a simple programming language for efficient and scalable machine
learning, we present FlashR, an interactive R-based programming framework that
executes R code in parallel and
out of core automatically for large-scale data analysis. FlashR stores large
vectors and matrices on SSDs and overrides many R functions in the R
\textit{base} package for these external-memory vectors and matrices.
As such, FlashR executes existing R code without or with little modification.
FlashR focuses on optimizations in a single machine (with multiple CPUs and
many cores) and scales matrix operations beyond memory capacity by 
utilizing solid-state drives (SSDs).  
%This paper does not address distributed matrix operations, but, 
%FlashR is suitable for executing the local part of a distributed 
%computation.   
Our evaluation shows that we can solve billion row, Internet-scale 
problems on a single thick node, which can prevent the complexity,
expense, and power consumption of distributed systems when they are
not strictly necessary \cite{hotos}.

%These machines typically have multiple processors with many CPU cores and
%a large amount of memory. They are also equipped with fast flash
%memory such as solid-state drives (SSDs) to further extend memory capacity.
%This conforms to the node design for supercomputers \cite{Ang14}.

% Why is it new/different/special?

To utilize the full capacity of a large parallel machine, we need to overcome
many technical challenges to move data from SSDs to CPU efficiently for matrix
computations,
notably the large performance disparities between CPU and memory and between
memory and SSDs. The ``memory gap'' \cite{Wilkes01} continues to grow, with 
the difference between CPU and DRAM performance increasing exponentially. 
There are also performance differences between
local and remote memory in a non-uniform memory architecture (NUMA), which are prevalent
in modern multiprocessor machines. 
RAM outperforms SSDs by an order of magnitude for both latency and throughput.
%Even though the I/O
%performance of SSDs has advanced to outperform hard drives by a large factor,
%FDespite advances in SSD I/O performance, 
%they remain an order of magnitude slower than RAM.
%Most matrix computation engines increase data movement,
%because they perform an operation on an entire input matrix prior to moving 
%to the next operation.
% RB -- 
%On the other hand, many analysis tasks are
%data-intensive. Matrix
%formulation further increases data movement between CPU and SSDs because
%a matrix computation framework typically performs an operation
%on the entire input matrices before moving to the next operation.
% As such,
%the performance of the data analysis tasks is usually limited by memory
%bandwidth instead of computing power.

FlashR evaluates expressions lazily and fuses operations aggressively
in a single parallel execution job to minimize data movement. FlashR
builds a directed acyclic graph (DAG) to represent a sequence of matrix
operations and grows a DAG as much as possible to increase the ratio of
computation to I/O. When evaluating the computation in a DAG, FlashR
performs two levels of matrix partitioning to reduce data movement in
the memory hierarchy. FlashR by default materializes
only the output matrices (leaf nodes) of a DAG and keeps materialized results in
memory in order to minimize data written to SSDs. FlashR streams
data from SSDs to maximize I/O throughput for most computation tasks.

% What are it's key features?

We implement multiple machine learning algorithms, including principal component
analysis, logistic regression and k-means, in FlashR. On a large parallel machine
with 48 CPU cores and fast SSDs, the out-of-core execution of these R implementations
achieves performance comparable to in-memory execution, while significantly
outperforming the same algorithms in H2O \cite{h2o} and Spark MLlib
\cite{spark}. FlashR effortlessly scales to datasets with billions
of data points and its out-of-core execution uses a negligible amount of memory
compared with the dataset size. In addition, FlashR executes the R functions
in the R MASS \cite{mass} package with little modification and outperforms
the execution of the same functions in Revolution R Open \cite{rro} by more
than an order of magnitude.

We believe that FlashR significantly lowers the requirements for writing
parallel and scalable implementations of machine learning algorithms; it also
offers new
design possibilities for data analysis clusters, replacing memory with larger
and cheaper SSDs and processing bigger problems on fewer nodes.
FlashR is released as an open-source project at http://flashx.io/.
