% What problem are you going to solve.

%The National Strategic Computing Initiative (NSCI \cite{NSCI}) puts forth
%a critical problem as we move to exascale: {\em ``Increasing coherence between
%the technology base used for 
%modeling and simulation and that used for data analytic computing.''}  
%A key challenge lies in providing statistical analysis and machine learning 
%tools that are simple and efficient.
%Simple tools need to be programmable, interactive, and extensible, 
%allowing scientists to encode and deploy complex algorithms. 
%Successful examples include R, SciPy, and Matlab.  Efficiency dictates that
%tools should  leverage modern HPC architectures, including scalable parallelism,
%high-speed networking, and fast I/O from memory and solid-state storage.

The I/O performance of solid-state drives (SSDs) has advanced tremendously.
Recent works \cite{flashgraph, gridgraph} demonstrate that SSD-based 
graph analysis matches the performance of state-of-the-art in-memory graph
analysis, while scaling to arbitrarily large datasets. This work extends
these findings to dense matrix operations: matrices are the most prevalent
and intuitive representation for machine learning and data analysis tasks.

%Unlike graph analysis, dense matrix based data analysis generates sequential data access patterns.

%that are based on dense matrix operations. 

%Matrix operations are an intuitive formulation for many computation tasks. 
%For example, we store samples
%collected from an experiment in a matrix with rows corresponding to samples
%and columns corresponding to attributes.
%Consequently, we express the computation on the data matrix with a sequence
%of matrix operations. Such a formulation simplifies the implementation of
%data analysis algorithms and is very intuitive for many machine learning
%and data analysis experts. 


Most of prior work on disk-based matrix computation focus on I/O complexity
and efficient I/O access of matrix operations~\cite{Toledo99, Quintana-Orti12}.
However, optimizing I/O alone is insufficient to achieve the best performance
from fast I/O. It is essential to move data efficiently both from
SSDs to memory and from memory to CPU caches in order to achieve performance
comparable to state-of-the-art in-memory implementations. 

%In today's big data era, we face the challenges in both the explosion of
%data volume and the increasing complexity of data analysis. Experiments,
%simulations and observations generate terabytes or
%even petabytes in many scientific and business areas. After collecting
%a massive amount of data, we often need to perform complex data analysis
%and machine learning techniques to extract value from the data. To handle
%the increasing volume size and effectively extract value from the data,
%the field of data mining and machine learning evolves rapidly and the community
%is developing many new algorithms to process large datasets.

% How have others addressed the problem?

% Why is it hard?

% What is the nature of your solution?
We present FlashMatrix, a programming framework that provides a high-level
functional programming interface and supports automatic
parallelization and out-of-core execution for large-scale data analysis.
FlashMatrix focuses on optimizations in a single machine (often with multiple
CPUs and many cores) and scales matrix operations beyond memory capacity by 
utilizing solid-state drives (SSDs).  
This paper does not address distributed matrix operations, but, 
FlashMatrix primitives are suitable for executing the local part of a distributed 
computation.   
Our evaluation does show that we can solve billion row, Internet-scale 
clustering problems on a single thick node, which can prevent the complexity,
expense, and power consumption of distributed systems when they are not strictly necessary
\cite{hotos}.



%This design choice conforms with a current trend of
%hardware design that scales up a single machine for high performance computing
%\cite{Ang14}.  It is also supported by 


%, including analysis of data stored on SSDs of I/O burst buffers \cite{burst}.

%These machines typically have multiple processors with many CPU cores and
%a large amount of memory. They are also equipped with fast flash
%memory such as solid-state drives (SSDs) to further extend memory capacity.
%This conforms to the node design for supercomputers \cite{Ang14}.

% Why is it new/different/special?

We overcome many technical challenges to move data from SSDs to CPU efficiently,
notably the large speed disparity between CPU and memory, as well as between
memory and SSDs. The speed disparity of CPU and DRAM has increased exponentially
over the past decades \cite{Wilkes01}. The speed disparity also exists between
local and remote memory in non-uniform memory architecture (NUMA), a prevalent
architecture for modern multiprocessor machines. Even though the I/O
performance of SSDs has advanced to outperform hard drives by a large factor,
they remain an order of magnitude slower than RAM.
%Most matrix computation engines increase data movement,
%because they perform an operation on an entire input matrix prior to moving 
%to the next operation.
% RB -- 
%On the other hand, many analysis tasks are
%data-intensive. Matrix
%formulation further increases data movement between CPU and SSDs because
%a matrix computation framework typically performs an operation
%on the entire input matrices before moving to the next operation.
% As such,
%the performance of the data analysis tasks is usually limited by memory
%bandwidth instead of computing power.

To achieve in-memory performance for out-of-core matrix operations,
FlashMatrix evaluates expressions lazily and fuses operations aggressively
in a single parallel execution job to minimize data movement. FlashMatrix
builds a directed acyclic graph (DAG) to represent a sequence of matrix
operations and grows a DAG as much as possible to increase the ratio of
computation to I/O. When evaluating the computation in a DAG, FlashMatrix
performs two levels of matrix partitioning to improve data utilization in
the memory hierarchy and reduce data movement between memory and SSDs
as well as between CPU and memory. FlashMatrix by default materializes
only \dz{sink matrices} in a DAG and keep their materialized results in
memory to minimize data written to SSDs. FlashMatrix streams
data from SSDs to maximize I/O throughput for most of computation tasks.

%One of the important questions is how SSDs make the design different.

% What are it's key features?

We implement multiple machine learning algorithms, including k-means \cite{kmeans}
and Gaussian Mixture Models \cite{gmm} in FlashMatrix with its R programming
interface. On a large parallel machine with 48
CPU cores and fast SSDs, the out-of-core execution of these R implementations
achieves performance comparable to the in-memory execution,
while significantly outperforming the same algorithms in Spark MLlib
\cite{spark}. FlashMatrix effortlessly scales to datasets with billions
of data points and its out-of-core execution uses a small fraction of
resources required by in-memory implementations. 
When running in a single thread, FlashMatrix 
outperforms the C and FORTRAN implementations of the R framework.
%In addition, 
%FlashMatrix achieves near-linear speedup in a multicore NUMA
%machine for all algorithms. 

We believe that FlashMatrix significantly lowers the requirements for writing parallel
and scalable implementations of data analysis algorithms; it also offers new
design possibilities for data analysis clusters, replacing memory with larger
and cheaper SSDs and processing bigger problems on fewer nodes.
